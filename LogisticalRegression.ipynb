{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification / Logistic Regression \n",
    "\n",
    "<B>Problem/use case</B> - when you have labelled data and want to build a classifier to detect things like:  \n",
    "- SPAM email filter based on presence of common spam words\n",
    "- M/F detector based on physical features\n",
    "- Something where you need a Y/N binary answer with indicative features\n",
    "\n",
    "### Lady Gaga Test\n",
    "I don't know if the Lady Gaga song detector Mike wrote is good, but we can try it out\n",
    "(fyi: https://github.com/qzmeng/ail/blob/master/training.ipynb)\n",
    "\n",
    "- There are 300 songs as samples\n",
    "- Songs have hundreds of words each\n",
    "- Total unique set of words is 4000+\n",
    "- If we consider each unique word as a feature, u have a matrix of 300 x 4000 as the Training Array\n",
    "\n",
    "The goal is to train a model using 230/300 songs, which are a mix of Lady Gaga and Clash songs.  The classifier should take in a new song and determine if its a Gaga song or not.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dive into Logistic Regression \n",
    "- <i>We probably need to briefly cover classification systems and labelled/supervised learning.. first ??</i>    \n",
    "\n",
    "- Term \"Regression\" is a bad name for this, i dont think it has anything to do w/ <i>regression</i>\n",
    "- this is used to classify data in machine lerning - ie, figure out patterns or classify things like:\n",
    "    - is email spam (based on features like word counts, word frequency, etc as feature vectors\n",
    "    - is a song from author Lady Gaga or Clash based on word frequencies\n",
    "- Term \"Logistic\" = due to logistic function or sigmoid function to keep values between 0-1\n",
    "\n",
    "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)\n",
    "\n",
    "\n",
    "- Idea is similar to Linear Regression, formalizing terminology better:\n",
    "    - Given n features and m training examples\n",
    "    - Given your features X<sup>1</sup>..X<sup>n</sup>\n",
    "    - Given your weights/parameters O<sup>1</sup>..O<sup>n</sup> - I use O instead of Theta since I don't know greek\n",
    "    - Given the training set X<sub>1</sub>..X<sub>m</sub>\n",
    "    - Given a hypothesis h<sub>O</sub>(x) encompassing your O-weights and X-features\n",
    "        - like:  h<sub>O</sub>(x) = O<sup>0</sup> + O<sup>1</sup>X<sup>1</sup> + O<sup>2</sup>X<sup>2</sup> + O<sup>n</sup>X<sup>n</sup>  \n",
    "        - where the features X<sup>1..n</sup> are words in the songs, weighted by O<sup>1..n</sup> +/- depending probabilities\n",
    "        - so imagine we are calculating something like h(x) = 0.50*('love' exists?) - 0.25*('nukes' exists) + ...\n",
    "        - a high \"score\" evaluates in the sigmoid as a 1 (positive).  Low negative evaluates to a 0 (negative)\n",
    "    - Wrap the h inside the sigmoid function\n",
    "\\begin{equation*}\n",
    "z = \\frac{1}{(1+e^-h)}\n",
    "\\end{equation*}\n",
    "    - Use the cost function C(O) = Y * (-log(h<sub>O</sub>(x))) + (1-Y)(log(h<sub>O</sub>(x)))\n",
    "    - Intuition with using -log(x) and log(x) as cost input to sigmoid is visible by viewing plots:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x102e17a90>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102e17b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt, math\n",
    "\n",
    "xs = range(1,120)\n",
    "xs = [x / 100.0 for x in xs]\n",
    "ys = [-math.log(x) for x in xs]\n",
    "plt.scatter(xs, ys)\n",
    "plt.xlabel('-log(x) functions')\n",
    "plt.plot([0,0],[-1,5])\n",
    "plt.plot([1,1],[-1,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking cost function - if Y=1, the cost function evaluates to 1 * (-log(h<sub>O</sub>(x))) - or the blue line. \n",
    "- For each X<super>n</super> training example where expect Y=1:\n",
    "    - if x is large, h(x) is 0 (or <0).  Or has 0 cost.   \n",
    "    - If x is small (or close to 0), the cost skyrockets to be very large (infinity)\n",
    "- Conversely if expect Y = 0, wrong answers have super high cost, right predictions have 0 cost.  \n",
    "<i>I'm not sure I get it, but I guess you would never have a wrong guess of 0 or 1 when expecting 1 or 0 ??</i>\n",
    "\n",
    "### Gradient Descent \n",
    "\n",
    "Again use GD to solve for O in example  \n",
    "- sub-hypothesis:  h<sub>O</sub>(x) = O<sup>0</sup> + O<sup>1</sup>X<sup>1</sup> + O<sup>2</sup>X<sup>2</sup> + O<sup>n</sup>X<sup>n</sup>  \n",
    "- hypothesis (wrapped in sygmoid:  g(h) = 1/(1-e^h))\n",
    "- cost function:  C(O) = Y * (-log(h<sub>O</sub>(x))) + (1-Y)(log(h<sub>O</sub>(x)))\n",
    "- As before, GD loop:\n",
    "    - O<sub>j</sub> = O<sub>j</sub> - learningRate * partialDeriv(C(O))\n",
    "    - for each O parameter \n",
    "- Until C(O) is minimized\n",
    "\n",
    "I finally upgraded the solver to handle an arbitrar hypothesis, cost, passing in the training data and an array of y answers.  Generic but still kind of ugly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic solver takes in hypothesis function, cost func, training matrix, theta array, yarray\n",
    "def grad_descent_sympy(hFunc, cFunc, trainingMatrix, yArr, step=0.01, loop_limit=50, step_limit=0.00001, batchSize=None):\n",
    "    guesses = [0.01]*len(trainingMatrix[0])    # initial guess for all \n",
    "    costChange = 1.0\n",
    "    if (batchSize == None):\n",
    "        batchSize = len(trainingMatrix)  #@todo can i set this in func param, or reduce to single expr\n",
    "    batchSize = min(len(trainingMatrix),batchSize)\n",
    "\n",
    "    # TODO do i really need these 2 here... pass them in?\n",
    "    ts = sp.symbols('t:'+str(len(trainingMatrix[0])))  #theta weight/parameter array\n",
    "    xs = sp.symbols('x:'+str(len(trainingMatrix[0])))  #feature array\n",
    "    \n",
    "    costF = evalSumF2(cFunc,xs,trainingMatrix,yArr)  # cost fun evaluted for testData\n",
    "    cost = 0.0+costF.subs(zip(ts,guesses))  \n",
    "\n",
    "    trainingMatrix = shuffle(trainingMatrix, random_state=0)\n",
    "\n",
    "    i=j=l=0\n",
    "    while (abs(costChange) > step_limit and l<loop_limit):  # outer loop batch chunk\n",
    "        i=j=k=0\n",
    "        k = j+batchSize if j+batchSize<len(trainingMatrix) else len(trainingMatrix)\n",
    "        dataBatch = trainingMatrix[j:k]\n",
    "        yBatch = yArr[j:k]\n",
    "\n",
    "        while (i < len(trainingMatrix)/batchSize):  # inner batch size loop, min 1x loop\n",
    "            for t,theta in enumerate(ts):\n",
    "                pd = evalPartialDeriv2(cFunc,theta,ts,xs,dataBatch,guesses,yBatch)\n",
    "                guesses[t] = guesses[t] - step * pd\n",
    "            previousCost = cost\n",
    "            cost = costF.subs(zip(ts,guesses))\n",
    "            costChange = previousCost-cost\n",
    "            log.warn('l=%d,bs=%d,costChange=%f,cost=%f, guesses=%s'%(l,batchSize, costChange,cost,gf(guesses)))\n",
    "\n",
    "            j = k\n",
    "            k = j+batchSize if j+batchSize<len(trainingMatrix) else len(trainingMatrix)\n",
    "            dataBatch = trainingMatrix[j:k]\n",
    "            yBatch = yArr[j:k]\n",
    "            i += 1\n",
    "            l += 1\n",
    "    return guesses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I back tested to solve linear regression, weee works.\n",
    "\n",
    "I tested with with the Gaga Dataset... and it was a failure (way too slow).\n",
    "A few observations:\n",
    "\n",
    "- The dataset has 300 training examples, but around 4000 features\n",
    "- Testing on 4000 features, or even 1000 is super super slow, like even 1 single pass to the GD solver takes hours on my MacPro\n",
    "- Most of the matrix are 0's since there are alot of useless words that are never re-used in subsequent datasets  \n",
    "\n",
    "So the short story is -- some fine tuning is needed in feature selection or something else.\n",
    "Thus I tried it out.. see <a href=\"FeatureEngineering.ipynb\">FeatureEngineering Notebook</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Rewriting the Gradient Solver !!\n",
    "\n",
    "At first i was trying to train the model by reducing features, but I realize I am wasting time w/ my slow GD4 solver.\n",
    "So time to upgrade.\n",
    "\n",
    "I upgraded yet again to v5 - main things:\n",
    "- removed use of sympy which was my way of using symbolic math, ie evaluate f=x*y and take derivative of f (f.diff) or evaluate x and y seperately.  \n",
    "- added matrix solver (had to re-learn linear algebra) so I could implement a vectorized solution \n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta = \\theta - \\frac{a}{(m)} * X^T*(g(X*\\theta) - \\vec{Y})\n",
    "\\end{equation*}\n",
    "\n",
    "where g = sigmoid function, and X*\\theta is X dotproduct \\theta.  \\theta and Y are vectors, X is a matrix.  Result is a vector of \\theta weights/guesses.  \n",
    "\n",
    "Net result:   Like <b>100x</b> - seems sympy evaluation was really slow, multiplying a 100x10 matrix is fast, evaluating 100 rows with 10 variables is surprisingly slow in sympy.   \n",
    "\n",
    "<i>It took some effort to make it generic to work for both a Linear Regression and Logistic Regression.  The difference is the cost/error functions, hence those are passed in. I'm tired of this code, I am moving on. </i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic solver, errorFunc(y,x) - can be lambda y,x: x-y, \n",
    "#                 costFunc(y,x) - can be sklearn.metric.log_loss|mean_squared_error or your own func to calc costSum \n",
    "#                 xArr (np.array - MxN), yArr (np.array [1xN]), returns [1xN]\n",
    "def grad_descent5(eFunc, cFunc, xArr, yArr, step=0.01, loop_limit=50, step_limit=0.00001, batchSize=None):\n",
    "    batchSize = len(xArr) if batchSize == None else min(len(xArr),batchSize)\n",
    "    guesses = [0.01]*len(xArr[0])  # initial guess for all \n",
    "    costChange = costSum = 1.0\n",
    "    xArr,yArr = shuffle(xArr, random_state=0), shuffle(yArr, random_state=0)  # perhaps should shuffle inside outer loop\n",
    "    \n",
    "    i=j=l=0\n",
    "    while (abs(costChange) > step_limit and l<loop_limit):  # outer loop batch chunk\n",
    "        i=j=k=0\n",
    "        k = j+batchSize if j+batchSize<len(xArr) else len(xArr)\n",
    "        xBatch,yBatch = xArr[j:k],yArr[j:k]\n",
    "        log.debug('outer - batch %d, j: %d, k: %d'%(len(xBatch),j,k))\n",
    "\n",
    "        while (i < len(xArr)/batchSize):  # inner batch size loop, min 1x loop\n",
    "            previousCost = costSum\n",
    "            xEval = np.dot(xBatch, guesses)  # array of X*0 evaluations\n",
    "            error = eFunc(yBatch, xEval)     # errorFunc could be x-y or sig(x)-y   \n",
    "            costSum = cFunc(yBatch,error+yBatch)  # cFunc log_loss, or mean_error_square, or custom.  xEval = error+yBatch in case of sig(x)\n",
    "            guesses = guesses - step * np.dot(xBatch.T, error) * 2.0/len(xBatch)  # ng std formula: 0 := 0 - a/m * X.T*(g(X*0) - Y)\n",
    "            costChange = previousCost-costSum\n",
    "            log.warn('l=%d,i=%d,bs=%d,costChange=%f,cost=%f, guesses=%s'%(l,i,batchSize, costChange,costSum,gf(guesses)))\n",
    "\n",
    "            j = k\n",
    "            k = j+batchSize if j+batchSize<len(xArr) else len(xArr)\n",
    "            xBatch,yBatch = xArr[j:k],yArr[j:k]\n",
    "            i += 1\n",
    "            l += 1\n",
    "    return guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance sidebar\n",
    "\n",
    "How much faster is this solver...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested old sympy solver vs matrix solver and its shocking\n",
    "\n",
    "| test | sympy | matrix | x increase |\n",
    "-------|-------|--------|------------|\n",
    "|5000 loop linear regression (2 features, SGD) | 138.14 sec | 0.93 sec | 251 x faster |\n",
    "|1000 loop linear regression (2 feature, BGD) | 44.20 sec | 0.18 sec | 148 x faster | \n",
    "|500 loop logistic regression (10 features, mini-BGD) | 394.25 sec | 0.14 sec | 2824 x faster |\n",
    "|250 loop logistic regression (20 features, mini-MBGD) | 1981.34 sec | 0.125 sec | 15,746 x faster |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Lady Gaga - Putting it all Together !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:mymodel guesses: ['-0.1770', '-0.2478', '-0.0418', '0.0168', '-0.1137', '-0.1550', '-0.0096', '-0.2135', '0.0143', '0.0156', '-1.1844', '0.0156', '0.2218', '-0.1891', '0.0151', '-0.0596', '-0.2198', '-0.0192', '0.1282', '0.1269', '...'] len 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw results ['-1.3789', '0.7054', '0.3861', '0.7072', '-1.6613', '-4.9136', '-7.1018', '-4.5589', '-5.5129', '-1.6713', '7.2668', '5.9865', '-2.7832', '0.0083', '-12.0435', '0.8534', '-0.0279', '-8.3808', '-1.3681', '-4.6447', '...'] \n",
      "sig results ['0.2012', '0.6694', '0.5953', '0.6698', '0.1596', '0.0073', '0.0008', '0.0104', '0.0040', '0.1583', '0.9993', '0.9975', '0.0582', '0.5021', '0.0000', '0.7013', '0.4930', '0.0002', '0.2029', '0.0095', '...']\n",
      "\n",
      "0|1 results ['0.0000', '1.0000', '1.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '1.0000', '0.0000', '1.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '...']\n",
      "\n",
      "diff vs y ['0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '-1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-1.0000', '0.0000', '-1.0000', '1.0000', '-1.0000', '-1.0000', '-1.0000', '0.0000', '...']\n",
      "\n",
      "mymodel errors: 17.0 / 84 = 0.202381\n"
     ]
    }
   ],
   "source": [
    "import featureEngineering as fe\n",
    "from logisticRegression import reduceFeatures\n",
    "from sklearn.utils import shuffle\n",
    "from gdsolvers import *\n",
    "log.getLogger().setLevel(log.ERROR)\n",
    "\n",
    "xMatrix,yArr,features,fnames = fe.getGagaData(maxrows=300,stopwords='english')   # get all songs \n",
    "xMatrix,yArr = shuffle(xMatrix, random_state=0), shuffle(yArr, random_state=0)   # shuffle\n",
    "partition = int(.70*len(yArr))            \n",
    "trainingMatrix = xMatrix[:partition]     # 70% training\n",
    "trainingY = yArr[:partition]\n",
    "testMatrix = xMatrix[partition:]         # 30% test [84 samples]\n",
    "testY = yArr[partition:]\n",
    "\n",
    "## train the model\n",
    "X = np.array(trainingMatrix)\n",
    "Y = trainingY\n",
    "X,rfeatures = reduceFeatures(X, Y, features, 500)   # reduce features from 4000+ -> 500\n",
    "gs = grad_descent5(lambda y,x: sigmoid(x)-y,sigmoidCost,X,Y,step=0.1,step_limit=-1,loop_limit=1000)\n",
    "log.error('mymodel guesses: %s len %d'%(gf(gs), len(gs)))\n",
    "\n",
    "## test the model\n",
    "df = pandas.DataFrame(testMatrix, columns=features)   # new df w/ column names\n",
    "X = df[rfeatures].as_matrix()                         # filter out only rfeatures\n",
    "testRes = np.dot(X, gs)                               # take test data and evaluate\n",
    "testResRound = [round(sigmoid(x),0) for x in testRes] # to sigmoid\n",
    "testDiffs = np.array(testResRound) - np.array(testY)  # diff vs Y\n",
    "print ('raw results %s '%(gf(testRes)))\n",
    "print ('sig results %s'%gf([sigmoid(x) for x in testRes]))\n",
    "print ('\\n0|1 results %s'%gf([round(sigmoid(x)) for x in testRes]))\n",
    "print ('\\ndiff vs y %s'%gf(testDiffs))\n",
    "print('\\nmymodel errors: %s / %s = %f'%(sum([abs(x) for x in testDiffs]),len(testY),sum([abs(x) for x in testDiffs])/len(testY)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of above\n",
    "Using 500 features (reduced w/ kMeans), we ran 70% training to produce the Theta (mymodel guesses array of weights).   \n",
    "\n",
    "<B>raw results</B>:  are the h(x) totals per feature   \n",
    "<B>sig results</B>:  are the g(h(x)) after running thru the sigmoid function to get 0.0-1.0 values  \n",
    "<B>0|1 results</B>:  rounding g(h(x)) to 0 or 1 to compare vs Y's\n",
    "<B>diff v y</B>:  the g(h(x)) - y results -- 0 means prediction was correct.  +1 = false pos, -1 false neg  \n",
    "\n",
    "In summary we have a <B><u>80% correct hit rate</u></B> (17 errors out of 84 test examples)\n",
    "\n",
    "Note Scikit does similar but I can't reproduce their theta/weight results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:scikit log_reg solution:['-0.1000', '-0.1516', '-0.0223', '0.0013', '-0.0758', '-0.1011', '-0.0039', '-0.1410', '0.0010', '0.0049', '-0.7145', '0.0049', '0.1536', '-0.0956', '0.0000', '-0.0450', '-0.1526', '-0.0090', '0.0868', '0.0838', '...'] len 500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0|1 results ['0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '1.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '...'] \n",
      "\n",
      "scikit total errors: 16.0 / 84 = 0.190476\n"
     ]
    }
   ],
   "source": [
    "# sklearn training\n",
    "X = np.asmatrix(trainingMatrix)\n",
    "Y = trainingY\n",
    "X, rfeatures = reduceFeatures(X, Y, features, 500)\n",
    "Y = trainingY\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X,Y)\n",
    "log.error('scikit log_reg solution:%s len %d '%(gf(log_reg.coef_[0]),len(log_reg.coef_[0])))\n",
    "    \n",
    "X = np.array(testMatrix)\n",
    "df = pandas.DataFrame(testMatrix, columns=features)   # new df w/ column names\n",
    "X = df[rfeatures].as_matrix()                # filter out only rfeatures\n",
    "Y = testY\n",
    "\n",
    "c = log_reg.predict(X)  # or can I use score()\n",
    "print ('0|1 results %s '%(gf(c)))\n",
    "print ('\\nscikit total errors: %s / %s = %f'%(sum([float(abs(x)) for x in (c-Y)]),len(c),sum([float(abs(x)) for x in (c-Y)])/len(Y)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
