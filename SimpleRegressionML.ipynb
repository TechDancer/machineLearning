{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Linear Regression\n",
    "\n",
    "Using the same example from Pierre, same data, but we can try to do this using basic machine learning.\n",
    "https://github.com/axiomiety/crashburn/blob/master/jupyter/simple_linear_regression.ipynb\n",
    "\n",
    "### Import the same datasets (and other setup libraries/functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (myutils.py, line 182)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"myutils.py\"\u001b[0;36m, line \u001b[0;32m182\u001b[0m\n\u001b[0;31m    pd = -----tbd------\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests, pandas, io\n",
    "import time, itertools\n",
    "import myutils\n",
    "\n",
    "url='http://www.stat.ufl.edu/~winner/data/brainhead.dat'\n",
    "data=requests.get(url)\n",
    "col_names=('gender', 'age_range', 'head_size', 'brain_weight')\n",
    "col_widths=[(8,8),(16,16),(21-24),(29-32)]\n",
    "df=pandas.read_fwf(io.StringIO(data.text), names=col_names, colspec=col_widths)\n",
    "df.head()\n",
    "\n",
    "dfs = [df, \n",
    "       churn(df,4),\n",
    "       churn(df,8),\n",
    "       churn(df,12),\n",
    "       churn(df,16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First before I start I want to do some sizing tests..  like a few magnitudes more data,\n",
    "so we make an array of dataFrames dfs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfs[0-4] #rows:\n",
      "237\n",
      "3792\n",
      "60672\n",
      "970752\n",
      "15532032\n"
     ]
    }
   ],
   "source": [
    "print('dfs[0-4] #rows:')\n",
    "for d in dfs:\n",
    "    print (d.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok I think these 5 samples is enough.\n",
    "\n",
    "## Analytical Method (and Performance) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>time</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237</td>\n",
       "      <td>5.974</td>\n",
       "      <td>39.671912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3792</td>\n",
       "      <td>4.681</td>\n",
       "      <td>810.083316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60672</td>\n",
       "      <td>1.999</td>\n",
       "      <td>30351.175588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>970752</td>\n",
       "      <td>62.855</td>\n",
       "      <td>15444.308329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15532032</td>\n",
       "      <td>1143.931</td>\n",
       "      <td>13577.769988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rows      time            rt\n",
       "0       237     5.974     39.671912\n",
       "1      3792     4.681    810.083316\n",
       "2     60672     1.999  30351.175588\n",
       "3    970752    62.855  15444.308329\n",
       "4  15532032  1143.931  13577.769988"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinregressResult(slope=0.26342933948939939, intercept=325.57342104944235, rvalue=0.79956970925429616, pvalue=5.9576308394065412e-54, stderr=0.012907433440886988)\n",
      "LinregressResult(slope=0.26342933948939917, intercept=325.57342104944314, rvalue=0.79956970925429593, pvalue=0.0, stderr=0.0032140617796317652)\n",
      "LinregressResult(slope=0.26342933948939917, intercept=325.57342104944314, rvalue=0.79956970925429582, pvalue=0.0, stderr=0.00080331675985773965)\n",
      "LinregressResult(slope=0.26342933948939984, intercept=325.57342104944075, rvalue=0.79956970925430071, pvalue=0.0, stderr=0.00020082608673381252)\n",
      "LinregressResult(slope=0.26342933948942276, intercept=325.57342104935742, rvalue=0.79956970925446968, pvalue=0.0, stderr=5.0206473196643748e-05)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "timings = {'rows':[],'time':[]}\n",
    "#timings.append(['data','time'])\n",
    "for d in dfs:\n",
    "    r = time_fn(linregress,d.head_size,d.brain_weight)\n",
    "    timings['rows'].append(d.shape[0])\n",
    "    timings['time'].append(r[1]*1000)\n",
    "t = pandas.DataFrame(timings)\n",
    "rt = t['rows']/t['time']\n",
    "t['rt'] = rt\n",
    "display(t)\n",
    "\n",
    "for d in dfs:\n",
    "    print(linregress(d.head_size,d.brain_weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The perf table shows it isn't bad.  It is scaling better than I thought.  \n",
    "\n",
    "Doesn't prove what everyone says, analytic solver good for < 10k or so size samples.  Even at 15m points it solves it fast.   (??)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Sidebar\n",
    "For fun, wanted to compare performance w/ Pierre's solution (vs the scipy library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325.57342104944223, 0.26342933948939945]\n",
      "[325.57342104944132, 0.26342933948939967]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>time</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237</td>\n",
       "      <td>17.438</td>\n",
       "      <td>13.591008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3792</td>\n",
       "      <td>195.380</td>\n",
       "      <td>19.408332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60672</td>\n",
       "      <td>3490.342</td>\n",
       "      <td>17.382824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rows      time         rt\n",
       "0    237    17.438  13.591008\n",
       "1   3792   195.380  19.408332\n",
       "2  60672  3490.342  17.382824"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pierre_solver (df):\n",
    "    brain_weight_sample_mean = df['brain_weight'].mean()\n",
    "    head_size_sample_mean = df['head_size'].mean()\n",
    "    b_1 = sum(df.apply(lambda r: (r['head_size']-head_size_sample_mean)*(r['brain_weight']-brain_weight_sample_mean), axis=1))\n",
    "    b_1 /= sum(df.apply(lambda r: (r['head_size']-head_size_sample_mean)**2, axis=1))\n",
    "    b_0 = brain_weight_sample_mean-b_1*head_size_sample_mean\n",
    "    return [b_0, b_1]\n",
    "\n",
    "# prove the solutions are about same:\n",
    "print (pierre_solver(dfs[0]))\n",
    "print (pierre_solver(dfs[1]))\n",
    "\n",
    "# run first 2 \n",
    "timings = {'rows':[],'time':[]}\n",
    "for d in dfs[:3]:\n",
    "    r = time_fn(pierre_solver,d)\n",
    "    timings['rows'].append(d.shape[0])\n",
    "    timings['time'].append(r[1]*1000)\n",
    "t = pandas.DataFrame(timings)\n",
    "rt = t['rows']/t['time']\n",
    "t['rt'] = rt\n",
    "display(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alot slower for obvious reasons (demo code, unoptimized etc).  It also shows as n grows > 1000's its infeasible to solve analytically... this crashes doing the 100k+ iteration (or never finishes).\n",
    "\n",
    "# Training Method \n",
    "\n",
    "Things needed for linear a regression line, of form f(x) = Ax + B (activation function)  \n",
    "- Find optimal A, B values & cost function J(A,B)  \n",
    "- Need partial derivatives for J(A,B) - dA and dB   \n",
    "- Iterate A = A + u * dA && B = B + u * dB  \n",
    "- Pretty much can start at random pt or 0,0 and do Gradient Descent  \n",
    "- Step until close to 0 - define step size (u) and when to stop  \n",
    "\n",
    "First a basic gradient descent example for a simple function f (x^2 - 2x + 1)\n",
    "The derivative of this is 2x - 2, start at x=0, and solve where y=0\n",
    "\n",
    "### Gradient Descent Dummy Sample (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= 0.2 y= 0.64\n",
      "x= 0.36 y= 0.4096\n",
      "x= 0.488 y= 0.262144\n",
      "x= 0.5904 y= 0.16777216\n",
      "x= 0.67232 y= 0.1073741824\n",
      "x= 0.737856 y= 0.068719476736\n",
      "x= 0.7902848 y= 0.043980465111\n",
      "x= 0.83222784 y= 0.0281474976711\n",
      "x= 0.865782272 y= 0.0180143985095\n",
      "x= 0.8926258176 y= 0.0115292150461\n",
      "x= 0.91410065408 y= 0.00737869762948\n",
      "error:  0.00737869762948\n",
      "x:  0.91410065408\n"
     ]
    }
   ],
   "source": [
    "from scipy.misc import derivative\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 2*x + 1\n",
    "\n",
    "def p(f, x):\n",
    "    return derivative(f,x)\n",
    "\n",
    "def grad_descent(f):\n",
    "    x = 0\n",
    "    step = 0.1\n",
    "    error = 0.5\n",
    "\n",
    "    # loop while error > err_lmt\n",
    "    while (error > 0.01):\n",
    "        x = x - step*(p(f,x)) \n",
    "        print ('x=',x, 'y=',f(x))\n",
    "        error = abs(f(x)-0)\n",
    "    print('error: ',error)\n",
    "    print('x: ',x)\n",
    "\n",
    "grad_descent(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "Training set: x[n] ~ df['head_size'] \n",
    "Solution set: y[n] ~ df['brain_weight']\n",
    "Hypothesis: h(x):  Ax + B  \n",
    "Cost P(h[x]) = P(A,B) = 1/n * sum( h(x[n]) - y[n] )^2  \n",
    "\n",
    "Consider P(A,B) = contour/3d of all costs of every combination A,B (0x+0, 1x+1, 0x+1, etc...)  \n",
    "Optimal is where P(A,B) = minimum  \n",
    "\n",
    "Start with guess Optimal P = A=1, B=1, h(x) = 1x+1  \n",
    "\n",
    "Optimal P(A,B) =  \n",
    " * A = A - step * dP/dA  <- initial guess +/- partial derivative of A (ie, movement of A parameter)      \n",
    " * B = B - step * dP/dA  <- initial guess +/- partial derivative of B (ie, movement of B parameter)  \n",
    "Repeat until close to step_limit  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init guess A: 1.000000, B: 1.000000\n",
      "init func: A*x + B, test size: 237\n",
      "init costF 0.00421940928270042*(2720.0*A + B - 955.0)**2 + 0.00421940928270042*(2773.0*A + \n",
      "init cost 5609738.70886076\n",
      "i=0,cost=3871290,A=0.135457,B=-1175.059072\n",
      "i=1,cost=2672945,A=0.851485,B=-192.217064\n",
      "i=2,cost=1846896,A=0.255257,B=-1001.816037\n",
      "i=3,cost=1277474,A=0.748530,B=-323.272384\n",
      "i=4,cost=884947,A=0.337256,B=-880.275431\n",
      "finished for rows,time(s) 15532032 14.868650000000002\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from sympy.core.compatibility import as_int\n",
    "import sympy.concrete.summations as sum\n",
    "import itertools\n",
    "\n",
    "# evaluate/calculate f with data sub for x and y (very slow iterative)\n",
    "def evalSumF(f,x,y,testData):\n",
    "    n=0\n",
    "    for _,d in testData.iterrows():  # global test data\n",
    "        n += f.subs(x,d.head_size).subs(y,d.brain_weight)\n",
    "    return n * (1.0/len(testData))\n",
    "\n",
    "# generate partial derivative of e, with respect to v, for testData (x,y) and evaluate\n",
    "def evalPartialDeriv(e,x,y,testData,v,guessV,o,guessO):\n",
    "    pc = evalSumF(sp.diff(e,v),x,y,testData)\n",
    "    pceval = pc.subs(v,guessV).subs(o,guessO)\n",
    "    return pceval\n",
    "\n",
    "# hard coded solver, start w/ guess, solve cost, iterate cost+/-partialDerivs\n",
    "def grad_descent2(testData):\n",
    "    guessA = guessB = 1.0   #initial guess y=1x+1\n",
    "\n",
    "    stepA = 0.00000005   #dif step for diff A,B ?\n",
    "    stepB = 0.25         #maybe normalize data first\n",
    "    step_limit = 0.0001  # when to stop, when cost stops changing\n",
    "    loop_limit = 5       # arbitrary max limits\n",
    "    costChange = 1.0\n",
    "\n",
    "    A,B,x,y = sp.symbols('A B x y')\n",
    "    f = A*x + B  # linear func y=mx+b\n",
    "    e = (f - y)**2  # error squared\n",
    "    print ('init guess A: %f, B: %f'%(guessA,guessB))\n",
    "    print ('init func: %s, test size: %d' %(str(f),testData.shape[0]))\n",
    "    \n",
    "    costF = evalSumF(e,x,y,testData)  # cost fun evaluted for testData\n",
    "    print('init costF',str(costF)[:80])\n",
    "    costEval = costF.subs(A,guessA).subs(B,guessB)  # cost evaluted for A B guess\n",
    "    print('init cost',costEval)\n",
    "\n",
    "    i=0  \n",
    "    while (abs(costChange) > step_limit and i<loop_limit):  # arbitrary limiter\n",
    "        pda = evalPartialDeriv(e,x,y,testData,A,guessA,B,guessB)\n",
    "        pdb = evalPartialDeriv(e,x,y,testData,B,guessB,A,guessA)\n",
    "        guessA = guessA - stepA * pda\n",
    "        guessB = guessB - stepB * pdb\n",
    "        previousCost = costEval\n",
    "        costEval = costF.subs(A,guessA).subs(B,guessB)\n",
    "        costChange = previousCost-costEval\n",
    "        print ('i=%d,cost=%d,A=%f,B=%f'%(i, int(costEval), guessA, guessB))\n",
    "        i=i+1\n",
    "    return guessA,guessB\n",
    "\n",
    "timings = []\n",
    "r = time_fn(grad_descent2,df)\n",
    "print ('finished for rows,time(s)',d.shape[0], r[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows it getting closer after 5 test iterations - cost is decreasing and A,B params are converging (?) towards solution of 0.26x + 325.   But this is super slow (32sec for 5 iterations)... and guess what, I ran this to near completion and it did converge after 1800 iterations!!  \n",
    "\n",
    "\n",
    "Other observations  \n",
    "- the initial A,B didn't matter as much as you'd think  \n",
    "- the step sizes for A,B really matter - wrong steps are you step widely over the solution back and forth.  Given A target range for a slope is small, and Y intercept is large, maybe we need to normalize/scale first?   Random fine tuning steps was required.  \n",
    "- it is super slow, in theory you can find a solution for large testData sets this way where analytic solutions wont work.  But my lame code using sympy is so slow.  Have to find a better library or more to matrix multiplication (not sure if I'll rewrite (gradient_descent3) using matrices but I should).  \n",
    "\n",
    "Visually showing the solution forming is interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations\n",
    "\n",
    "Gradient Descent (aka Batch Gradient Descent), while in theory faster than solving via the the normal equation (examples to come later), or the builtin solvers when the # of features or size of training set is very large, is still very slow overall.  *In Batch GD, every step takes the cost down and steps towards the final solution, since solving the derivatives for all test data always points you in the right direction*\n",
    " \n",
    "There are 2 optimizations to the GD algo above  \n",
    "\n",
    "1.  Stochastic GD   \n",
    "    + Step A - Add a shuffle to the test data set\n",
    "    + Step B - instead of solving the partial derivative for all test data, solve for just 1 and take a step  \n",
    "    + *Solving derivatives for just random testData doesn't always take your cost down, but testing and logic shows it   jumps around a little but get you there quickly*  \n",
    "    + *Each step is way faster like 1/n times faster where n is size of test data*  \n",
    "\n",
    "\n",
    "2.  Mini-Batch GD  \n",
    "    * Step A - Add a shuffle to the test data set\n",
    "    * Step B - instead of solving the partial derivative for all test data, solve for a small batch like 10 and take a  step  \n",
    "    * *Solving a small batch makes it more likely to step in the right directin (cost down), and testing agrees*\n",
    "    * *Each step is also way faster like batchSize/n times faster* \n",
    "\n",
    "Sample results of a test run of 1,000 iterations of standard B-GD, S-GD, and MB-GD (size 10) for test data size of 200\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><td>*</td><td>B-GD</td><td>MB-GD</td><td>S-GD</td></tr>\n",
    "    <tr><td>A</td><td>0.28</td><td>0.27</td><td>0.19</td></tr>\n",
    "    <tr><td>B</td><td>253</td><td>365</td><td>477</td></tr>\n",
    "    <tr><td>Timing(s)</td><td>1899</td><td>702</td><td>582</td></tr>\n",
    "</table>\n",
    "\n",
    "(Reminder target solutions are A = 0.26, B=325)\n",
    "\n",
    "I should graph the results because its not evident from above that the results of MB-GD and S-GD actually jump around close  to the solution for some time but do not converge very well, while B-GD always moves slowly towards the final answer.   Some fine tuning required I think, as MB-GD and S-GD don't need 1,000 iterations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation - Linear Algebra\n",
    "\n",
    "Final note, the Normal Equation can be used which is really quite easy as shown below using numpy to calculate\n",
    "\n",
    "![Normal Equation Image](https://csharpcorner-mindcrackerinc.netdna-ssl.com/article/normal-equation-implementation-from-scratch-in-python/Images/image001.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -2.48689958e-14],\n",
       "        [  1.00000000e+01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "\n",
    "X = np.matrix('1,1;1,2;1,4;1,5')     # make sample matrix of test X head size data, note u have to put a dummy 1 column\n",
    "Y = np.matrix('10;20;40;50')         # make sample matrix of correlated Y(X) brain weight data\n",
    "\n",
    "(X.T.dot(X)).I.dot(X.T).dot(Y)       # We can expect a simple line of Y=10x+0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more realistic, we'll solve for the real data set as well using the original examples of data from http://www.stat.ufl.edu/~winner/data/brainhead.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  3.25573421e+02,   2.63429339e-01]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pandas.read_fwf(io.StringIO(data.text), names=col_names, colspec=col_widths)\n",
    "\n",
    "def normal_solver(d):\n",
    "    # some setup to make a ndarray\n",
    "    X = pandas.DataFrame({'Bias' : 1,'head_size' : d['head_size']})\n",
    "    Y = d['brain_weight']\n",
    "    X = np.asmatrix(X.as_matrix())\n",
    "    Y = Y.as_matrix()\n",
    "    return (X.T.dot(X)).I.dot(X.T).dot(Y)\n",
    "\n",
    "normal_solver(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look good, Y = 0.26x + 325 as expected !\n",
    "\n",
    "For kicks I timed the normal equation... and interesting.. linregress is way faster, scales better (not surprised)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linegresstime</th>\n",
       "      <th>normaltime</th>\n",
       "      <th>rows</th>\n",
       "      <th>rateA</th>\n",
       "      <th>rateB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.016</td>\n",
       "      <td>7.257</td>\n",
       "      <td>237</td>\n",
       "      <td>32.658123</td>\n",
       "      <td>233.267717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.629</td>\n",
       "      <td>17.338</td>\n",
       "      <td>3792</td>\n",
       "      <td>218.710347</td>\n",
       "      <td>6028.616852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.521</td>\n",
       "      <td>26.084</td>\n",
       "      <td>60672</td>\n",
       "      <td>2326.023616</td>\n",
       "      <td>2956.581063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.664</td>\n",
       "      <td>119.651</td>\n",
       "      <td>970752</td>\n",
       "      <td>8113.195878</td>\n",
       "      <td>12499.381953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1280.261</td>\n",
       "      <td>1676.283</td>\n",
       "      <td>15532032</td>\n",
       "      <td>9265.757632</td>\n",
       "      <td>12131.926224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   linegresstime  normaltime      rows        rateA         rateB\n",
       "0          1.016       7.257       237    32.658123    233.267717\n",
       "1          0.629      17.338      3792   218.710347   6028.616852\n",
       "2         20.521      26.084     60672  2326.023616   2956.581063\n",
       "3         77.664     119.651    970752  8113.195878  12499.381953\n",
       "4       1280.261    1676.283  15532032  9265.757632  12131.926224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timings = {'rows':[],'normaltime':[],'linegresstime':[]}\n",
    "for d in dfs:\n",
    "    r  = time_fn(normal_solver,d)\n",
    "    r2 = time_fn(linregress,d.head_size,d.brain_weight)\n",
    "    timings['rows'].append(d.shape[0])\n",
    "    timings['normaltime'].append(r[1]*1000)\n",
    "    timings['linegresstime'].append(r2[1]*1000)\n",
    "t = pandas.DataFrame(timings)\n",
    "rt = t['rows']/t['normaltime']\n",
    "t['rateA'] = rt\n",
    "rt2 = t['rows']/t['linegresstime']\n",
    "t['rateB'] = rt2\n",
    "display(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
