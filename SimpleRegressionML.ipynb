{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Linear Regression\n",
    "\n",
    "Using the same example from Pierre, same data, but we can try to do this using basic machine learning.\n",
    "https://github.com/axiomiety/crashburn/blob/master/jupyter/simple_linear_regression.ipynb\n",
    "\n",
    "### Import the same datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age_range</th>\n",
       "      <th>head_size</th>\n",
       "      <th>brain_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4512</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3738</td>\n",
       "      <td>1297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4261</td>\n",
       "      <td>1335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3777</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4177</td>\n",
       "      <td>1590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  age_range  head_size  brain_weight\n",
       "0       1          1       4512          1530\n",
       "1       1          1       3738          1297\n",
       "2       1          1       4261          1335\n",
       "3       1          1       3777          1282\n",
       "4       1          1       4177          1590"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, pandas, io\n",
    "\n",
    "url='http://www.stat.ufl.edu/~winner/data/brainhead.dat'\n",
    "data=requests.get(url)\n",
    "col_names=('gender', 'age_range', 'head_size', 'brain_weight')\n",
    "col_widths=[(8,8),(16,16),(21-24),(29-32)]\n",
    "df=pandas.read_fwf(io.StringIO(data.text), names=col_names, colspec=col_widths)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First before I start I want to do some sizing tests..  like a few magnitudes more data,\n",
    "so we make an array of dataFrames dfs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 4)\n",
      "(3792, 4)\n",
      "(60672, 4)\n",
      "(970752, 4)\n",
      "(15532032, 4)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def churn(d, n):\n",
    "    for _ in itertools.repeat(None, n):\n",
    "        d = d.append(d)\n",
    "    return d\n",
    "\n",
    "dfs = [df, \n",
    "       churn(df,4),\n",
    "       churn(df,8),\n",
    "       churn(df,12),\n",
    "       churn(df,16)]\n",
    "       \n",
    "for d in dfs:\n",
    "    print (d.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok I think these 5 samples is enough.\n",
    "\n",
    "## Analytical Method \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433 µs ± 61.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "511 µs ± 72.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.41 ms ± 57.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "17.8 ms ± 981 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "446 ms ± 16.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "LinregressResult(slope=0.26342933948939939, intercept=325.57342104944235, rvalue=0.79956970925429616, pvalue=5.9576308394065412e-54, stderr=0.012907433440886988)\n",
      "LinregressResult(slope=0.26342933948939917, intercept=325.57342104944314, rvalue=0.79956970925429593, pvalue=0.0, stderr=0.0032140617796317652)\n",
      "LinregressResult(slope=0.26342933948939917, intercept=325.57342104944314, rvalue=0.79956970925429582, pvalue=0.0, stderr=0.00080331675985773965)\n",
      "LinregressResult(slope=0.26342933948939984, intercept=325.57342104944075, rvalue=0.79956970925430071, pvalue=0.0, stderr=0.00020082608673381252)\n",
      "LinregressResult(slope=0.26342933948942276, intercept=325.57342104935742, rvalue=0.79956970925446968, pvalue=0.0, stderr=5.0206473196643748e-05)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import linregress\n",
    "import timeit\n",
    "\n",
    "for d in dfs:\n",
    "    %%timeit linregress(d.head_size,d.brain_weight)\n",
    "\n",
    "for d in dfs:\n",
    "    print(linregress(d.head_size,d.brain_weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows growth eyeballing it ...  data size {10k, 100k, 1m} -> {6ms, 60ms, 1100ms} seems O(n^2) at least, i can't actually calculate any higher datasets w/ this tutorial - run out of memory!  Lol\n",
    "\n",
    "Somewhat proves what everyone says, analytic solver good for < 10k or so size samples (and we are using only a simple linear model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Sidebar\n",
    "For fun, wanted to compare performance w/ Pierre's solution (vs the scipy library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325.57342104944223, 0.26342933948939945]\n",
      "[325.57342104944132, 0.26342933948939967]\n",
      "52.7 ms ± 4.27 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "763 ms ± 44.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "12.3 s ± 449 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "3min 24s ± 7.24 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "def pierre_solver (df):\n",
    "    brain_weight_sample_mean = df['brain_weight'].mean()\n",
    "    head_size_sample_mean = df['head_size'].mean()\n",
    "\n",
    "    b_1 = sum(df.apply(lambda r: (r['head_size']-head_size_sample_mean)*(r['brain_weight']-brain_weight_sample_mean), axis=1))\n",
    "    b_1 /= sum(df.apply(lambda r: (r['head_size']-head_size_sample_mean)**2, axis=1))\n",
    "\n",
    "    b_0 = brain_weight_sample_mean-b_1*head_size_sample_mean\n",
    "    return [b_0, b_1]\n",
    "\n",
    "#    df['error'] = df.apply(lambda r: r['brain_weight'] - b_0 - b_1*r['head_size'], axis=1)\n",
    "#    rss = sum(df['error']**2)\n",
    "#    print(rss)\n",
    "\n",
    "# prove the solutions are about same:\n",
    "print (pierre_solver(dfs[0]))\n",
    "print (pierre_solver(dfs[1]))\n",
    "\n",
    "for d in dfs:\n",
    "    %%timeit r = pierre_solver(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alot slower for obvious reasons (demo code, unoptimized etc).  It also shows as n grows > 1000's its infeasible to solve analytically... this crashes doing the 100k+ iteration (or never finishes).\n",
    "\n",
    "# Training Method \n",
    "\n",
    "Things needed for linear a regression line, of form f(x) = Ax + B (activation function)  \n",
    "- Find optimal A, B values & cost function J(A,B)  \n",
    "- Need partial derivatives for J(A,B) - dA and dB   \n",
    "- Iterate A = A + u * dA && B = B + u * dB  \n",
    "- Pretty much can start at random pt or 0,0 and do Gradient Descent  \n",
    "- Step until close to 0 - define step size (u) and when to stop  \n",
    "\n",
    "First a basic gradient descent example for a simple function f (x^2 - 2x + 1)\n",
    "The derivative of this is 2x - 2, start at x=0, and solve where y=0\n",
    "\n",
    "### Gradient Descent Dummy Sample (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= 0.2 y= 0.64\n",
      "x= 0.36 y= 0.4096\n",
      "x= 0.488 y= 0.262144\n",
      "x= 0.5904 y= 0.16777216\n",
      "x= 0.67232 y= 0.1073741824\n",
      "x= 0.737856 y= 0.068719476736\n",
      "x= 0.7902848 y= 0.043980465111\n",
      "x= 0.83222784 y= 0.0281474976711\n",
      "x= 0.865782272 y= 0.0180143985095\n",
      "x= 0.8926258176 y= 0.0115292150461\n",
      "x= 0.91410065408 y= 0.00737869762948\n",
      "error:  0.00737869762948\n",
      "x:  0.91410065408\n"
     ]
    }
   ],
   "source": [
    "from scipy.misc import derivative\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 2*x + 1\n",
    "\n",
    "def p(f, x):\n",
    "    return derivative(f,x)\n",
    "\n",
    "def grad_descent(f):\n",
    "    x = 0\n",
    "    step = 0.1\n",
    "    error = 0.5\n",
    "\n",
    "    # loop while error > err_lmt\n",
    "    while (error > 0.01):\n",
    "        x = x - step*(p(f,x)) \n",
    "        print ('x=',x, 'y=',f(x))\n",
    "        error = abs(f(x)-0)\n",
    "    print('error: ',error)\n",
    "    print('x: ',x)\n",
    "\n",
    "grad_descent(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "Training set: x[n] ~ df['head_size'] \n",
    "Solution set: y[n] ~ df['brain_weight']\n",
    "Hypothesis: h(x):  Ax + B  \n",
    "Cost P(h[x]) = P(A,B) = 1/n * sum( h(x[n]) - y[n] )^2  \n",
    "\n",
    "Consider P(A,B) = contour/3d of all costs of every combination A,B (0x+0, 1x+1, 0x+1, etc...)  \n",
    "Optimal is where P(A,B) = minimum  \n",
    "\n",
    "Start with guess Optimal P = A=1, B=1, h(x) = 1x+1  \n",
    "\n",
    "Optimal P(A,B) =  \n",
    " * A = A - step * dP/dA  <- initial guess +/- partial derivative of A (ie, movement of A parameter)      \n",
    " * B = B - step * dP/dA  <- initial guess +/- partial derivative of B (ie, movement of B parameter)  \n",
    "Repeat until close to step_limit  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5609738.7088607587"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def costP(df,A,B):\n",
    "    n = 0\n",
    "    for _,x in df.iterrows():  # global test data\n",
    "        n += (x[2]*A + B - x[3])**2\n",
    "    return 1/len(df) * n \n",
    "    \n",
    "def partialDeriv(p, v):  # partial deriv of function p w/ respect to v    \n",
    "    return 0.088 # tbd\n",
    "\n",
    "def grad_descent2():\n",
    "    guessA = guessB = 1   # h(x) = Ax+B = x+1\n",
    "    testData = df\n",
    "    step - 0.05\n",
    "    step_limit = 0.01 # when to stop\n",
    "    changeA = changeB = 1\n",
    "    \n",
    "    while (changeA < step_limit) and (changeB < step_limit):\n",
    "        changeA = step * partialDeriv(costP,guessA)\n",
    "        changeB = step * partialDeriv(costP,guessB)\n",
    "        guessA = guessA - changeA\n",
    "        guessB = guessB - changeB\n",
    "        print (guessA, guessB)\n",
    "    return guessA,guessB\n",
    "\n",
    "costP(df, 1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row5 = df.head(5)\n",
    "len(row5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
