{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification / Logistic Regression \n",
    "\n",
    "<B>Problem/use case</B> - when you have labelled data and want to build a classifier to detect things like:  \n",
    "- SPAM email filter based on presence of common spam words\n",
    "- M/F detector based on physical features\n",
    "- Something where you need a Y/N binary answer with indicative features\n",
    "\n",
    "### Lady Gaga Test\n",
    "I don't know if the Lady Gaga song detector Mike wrote is good, but we can try it out\n",
    "(fyi: https://github.com/qzmeng/ail/blob/master/training.ipynb)\n",
    "\n",
    "- There are 300 songs as samples\n",
    "- Songs have hundreds of words each\n",
    "- Total unique set of words is 4000+\n",
    "- If we consider each unique word as a feature, u have a matrix of 300 x 4000 as the Training Array\n",
    "\n",
    "The goal is to train a model using 230/300 songs, which are a mix of Lady Gaga and Clash songs.  The classifier should take in a new song and determine if its a Gaga song or not.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dive into Logistic Regression \n",
    "- <i>We probably need to briefly cover classification systems and labelled/supervised learning.. first ??</i>    \n",
    "\n",
    "- Term \"Regression\" is a bad name for this, i dont think it has anything to do w/ <i>regression</i>\n",
    "- this is used to classify data in machine lerning - ie, figure out patterns or classify things like:\n",
    "    - is email spam (based on features like word counts, word frequency, etc as feature vectors\n",
    "    - is a song from author Lady Gaga or Clash based on word frequencies\n",
    "- Term \"Logistic\" = due to logistic function or sigmoid function to keep values between 0-1\n",
    "\n",
    "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)\n",
    "\n",
    "\n",
    "- Idea is similar to Linear Regression, formalizing terminology better:\n",
    "    - Given n features and m training examples\n",
    "    - Given your features X<sup>1</sup>..X<sup>n</sup>\n",
    "    - Given your weights/parameters O<sup>1</sup>..O<sup>n</sup> - I use O instead of Theta since I don't know greek\n",
    "    - Given the training set X<sub>1</sub>..X<sub>m</sub>\n",
    "    - Given a hypothesis h<sub>O</sub>(x) encompassing your O-weights and X-features\n",
    "        - like:  h<sub>O</sub>(x) = O<sup>0</sup> + O<sup>1</sup>X<sup>1</sup> + O<sup>2</sup>X<sup>2</sup> + O<sup>n</sup>X<sup>n</sup>  \n",
    "        - where the features X<sup>1..n</sup> are words in the songs, weighted by O<sup>1..n</sup> +/- depending probabilities\n",
    "        - so imagine we are calculating something like h(x) = 0.50*('love' exists?) - 0.25*('nukes' exists) + ...\n",
    "        - a high \"score\" evaluates in the sigmoid as a 1 (positive).  Low negative evaluates to a 0 (negative)\n",
    "    - Wrap the h inside the sigmoid function\n",
    "\\begin{equation*}\n",
    "z = \\frac{1}{(1-e^h)}\n",
    "\\end{equation*}\n",
    "    - Use the cost function C(O) = Y * (-log(h<sub>O</sub>(x))) + (1-Y)(log(h<sub>O</sub>(x)))\n",
    "    - Intuition with using -log(x) and log(x) as cost input to sigmoid is visible by viewing plots:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x102e17a90>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102e17b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt, math\n",
    "\n",
    "xs = range(1,120)\n",
    "xs = [x / 100.0 for x in xs]\n",
    "ys = [-math.log(x) for x in xs]\n",
    "plt.scatter(xs, ys)\n",
    "plt.xlabel('-log(x) functions')\n",
    "plt.plot([0,0],[-1,5])\n",
    "plt.plot([1,1],[-1,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking cost function - if Y=1, the cost function evaluates to 1 * (-log(h<sub>O</sub>(x))) - or the blue line. \n",
    "- For each X<super>n</super> training example where expect Y=1:\n",
    "    - if x is large, h(x) is 0 (or <0).  Or has 0 cost.   \n",
    "    - If x is small (or close to 0), the cost skyrockets to be very large (infinity)\n",
    "- Conversely if expect Y = 0, wrong answers have super high cost, right predictions have 0 cost.  \n",
    "<i>I'm not sure I get it, but I guess you would never have a wrong guess of 0 or 1 when expecting 1 or 0 ??</i>\n",
    "\n",
    "### Gradient Descent \n",
    "\n",
    "Again use GD to solve for O in example  \n",
    "- sub-hypothesis:  h<sub>O</sub>(x) = O<sup>0</sup> + O<sup>1</sup>X<sup>1</sup> + O<sup>2</sup>X<sup>2</sup> + O<sup>n</sup>X<sup>n</sup>  \n",
    "- hypothesis (wrapped in sygmoid:  g(h) = 1/(1-e^h))\n",
    "- cost function:  C(O) = Y * (-log(h<sub>O</sub>(x))) + (1-Y)(log(h<sub>O</sub>(x)))\n",
    "- As before, GD loop:\n",
    "    - O<sub>j</sub> = O<sub>j</sub> - learningRate * partialDeriv(C(O))\n",
    "    - for each O parameter \n",
    "- Until C(O) is minimized\n",
    "\n",
    "I finally upgraded the solver to handle an arbitrar hypothesis, cost, passing in the training data and an array of y answers.  Generic !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic solver takes in hypothesis function, cost func, training matrix, theta array, yarray\n",
    "def grad_descent4(hFunc, cFunc, trainingMatrix, yArr):\n",
    "    guesses = [0.1]*len(trainingMatrix[0])    # initial guess for all \n",
    "    step = 0.05          # init step\n",
    "    step_limit = 0.00001   # when to stop, when cost stops changing\n",
    "    loop_limit = 50      # arbitrary max limits\n",
    "    costChange = 1.0\n",
    "\n",
    "    # TODO do i really need these 2 here... pass them in?\n",
    "    ts = sp.symbols('t:'+str(len(trainingMatrix[0])))  #theta weight/parameter array\n",
    "    xs = sp.symbols('x:'+str(len(trainingMatrix[0])))  #feature array\n",
    "    \n",
    "    log.warn('init guesses %s',str(guesses))\n",
    "    log.warn('init func: %s, training size: %d' %(str(hFunc),trainingMatrix.shape[0]))\n",
    "    log.debug('ts: %s / xs: %s',ts,xs)\n",
    "\n",
    "    costF = evalSumF2(cFunc,xs,trainingMatrix,yArr)  # cost fun evaluted for testData\n",
    "    cost = 0.0+costF.subs(zip(ts,guesses))  \n",
    "    log.warn('init cost: %f, costF %s',cost,str(costF)) # show first 80 char of cost evaluation\n",
    "\n",
    "    i=0  \n",
    "    while (abs(costChange) > step_limit and i<loop_limit):  # arbitrary limiter\n",
    "        for j,theta in enumerate(ts):\n",
    "            pd = evalPartialDeriv2(cFunc,theta,ts,xs,trainingMatrix,guesses,yArr)\n",
    "            guesses[j] = guesses[j] - step * pd\n",
    "        previousCost = cost\n",
    "        cost = costF.subs(zip(ts,guesses))\n",
    "        costChange = previousCost-cost\n",
    "        log.warn('i=%d,costChange=%f,cost=%f, guesses=%s'%(i, costChange,cost,str(guesses)))\n",
    "        i=i+1\n",
    "    return guesses\n",
    "\n",
    "#setup\n",
    "trainingMatrix,yArr,labels = getGagaData(maxrows=300,maxfeatures=20)\n",
    "ts = sp.symbols('t:'+str(len(trainingMatrix[0])))  #theta weight/parameter array\n",
    "xs = sp.symbols('x:'+str(len(trainingMatrix[0])))  #feature array\n",
    "\n",
    "c,g,h,y = sp.symbols('c g h y')\n",
    "h = (sp.Matrix([ts])*sp.Matrix(xs))[0] # multipy ts's * xs's ( ts * xs.T )\n",
    "g = 1 / (1+mp.e**-h)   # wrap h in sigmoid\n",
    "c = -y*sp.log(g) - (1-y)*sp.log(1-g)  # cost func of single sample\n",
    "\n",
    "grad_descent4(g,c,trainingMatrix,yArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested with with Mike Eng's Gaga vs Clash Data.   https://github.com/qzmeng/ail/blob/master/training.ipynb\n",
    "\n",
    "A few observations:\n",
    "\n",
    "- The dataset has 300 training examples, but around 4000 features\n",
    "- Testing on 4000 features, or even 1000 is super super slow, like even 1 single pass to the GD solver takes hours on my MacPro\n",
    "- Most of the matrix are 0's since there are alot of useless words that are never re-used in subsequent datasets  \n",
    "\n",
    "So the short story is -- some fine tuning is needed in feature selection or something else.\n",
    "Thus I tried it out.. see <a href=\"FeatureEngineering.ipynb\">FeatureEngineering Notebook</a>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
