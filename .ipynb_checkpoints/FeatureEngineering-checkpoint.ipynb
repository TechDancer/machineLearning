{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature Engineering 1.0\n",
    "\n",
    "As I started playing w/ logistic regression to build a Lada Gaga song detector, I found that the # of features is too large to compute.  Moreso, the feature grid was mostly sparse and mostly useless since it was based on a basic word count vectorization.  (See the nice example using Naive Bayes from Mike: https://github.com/qzmeng/ail/blob/master/training.ipynb)\n",
    "\n",
    "There is some good scikit info on managing this:\n",
    "http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py  \n",
    "\n",
    "This is a survey of my findings\n",
    "\n",
    "- Using 300 songs (1/2 Lady Gaga 1/2 Clash), we counted words\n",
    "- 4000+ words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {margin-left: 0 !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>table {margin-left: 0 !important;}</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tons of words only used once, a few used many times, the rest are common words counted alot of times \n",
    "\n",
    "| Times Counted | Frequency | Meaning\n",
    "|---------------|-----------|----\n",
    "|1: | 1992   |  1992 words appeared once\n",
    "|2: | 572\n",
    "|3: | 296\n",
    "|4: | 212    | 212 words appeared 4 times\n",
    "|5: | 123   \n",
    "|.. | ..\n",
    "|25-600 | ~1  | Many diff words appeared a many times\n",
    "\n",
    "\n",
    "\n",
    "Most frequent words are maybe useless conjugation:\n",
    "\n",
    "| Word [Index] | count | Word [Index] | count |\n",
    "|--------  |------  |----------- |----  |\n",
    "|in  [1155]    |           ct: 321      | on  [1581]    |         ct:329       |\n",
    "|love  [1356]     |          ct: 331     |    that  [2288]         |      ct:366       |\n",
    "|oh  [1573]      |           ct: 394     |    be  [166]          |        ct:394       |\n",
    "|your  [2612]    |           ct: 487     |     my  [1512]        |         ct:489      | \n",
    "|to  [2331]       |          ct: 495    |     and  [69]          |        ct:497     |  \n",
    "|it  [1185]      |           ct: 550    |     me  [1414]       |          ct:589    |   \n",
    "|the  [2289]     |           ct: 980   |      you  [2609]        |        ct:1260  |\n",
    "\n",
    "\n",
    "Lots of words only used once:  \n",
    "\n",
    "| Word [Index] | count | Word [Index] | count |\n",
    "|--------  |------  |----------- |----  |\n",
    "|typical  [2399]    |       ct: 1   |       six  [2046]           |    ct:1       |  \n",
    "|hits  [1080]        |      ct: 1    |      photo  [1674]          |   ct:1       |  \n",
    "|screaming  [1940]  |       ct: 1    |      atmosphere  [114]      |   ct:1      |   \n",
    "|fit  [814]         |       ct: 1    |      rootiful  [1877]      |    ct:1       |  \n",
    "\n",
    "Useful words maybe in middle of count frequency?\n",
    "\n",
    "| Word [Index] | count | Word [Index] | count |\n",
    "|--------  |------  |----------- |----  |\n",
    "|superstar  [2215]    |     ct: 57      |   fame  [746]         |      ct:57        \n",
    "|time  [2328]         |     ct: 58        | think  [2300]        |     ct:58        \n",
    "|now  [1558]          |     ct: 59      |   tonight  [2341]      |     ct:60        \n",
    "|gaga  [884]            |   ct: 61         |  heart  [1042]      |       ct:61  \n",
    "\n",
    "One problem is we are counting total # of words.  I imagine in a song, the same words repeat alot so imagine if the chorus is \"love love love... all you need is love love love\" you are getting like 20 hits from one song.    \n",
    "  \n",
    "### Some more analysis using Pandas.DataFrame\n",
    "\n",
    "I found its way easier to do stuff in DataFrames than with Numpy arrays.  \n",
    "One thing I did is created this table:  [word, count, countFiles]\n",
    " - created matrix of word counts in + gaga files:  \n",
    " - created matrix of word counts in - gaga files:  \n",
    " - merged the tables (outer-join), and 0-filled Nulls (pandas.merge())\n",
    " - created 2 derived columns, calculating +/- of words more frequent/less frequent in gaga vs non-gaga files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "top/bot # variance of # words *2nd to last column*\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>gct</th>\n",
       "      <th>gfct</th>\n",
       "      <th>nct</th>\n",
       "      <th>nfct</th>\n",
       "      <th>gct-delta</th>\n",
       "      <th>gfct-delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>the</td>\n",
       "      <td>1154.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-174.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>they</td>\n",
       "      <td>137.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-94.0</td>\n",
       "      <td>-34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>an</td>\n",
       "      <td>110.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>he</td>\n",
       "      <td>143.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>war</td>\n",
       "      <td>47.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>-15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>your</td>\n",
       "      <td>155.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>oh</td>\n",
       "      <td>35.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>my</td>\n",
       "      <td>119.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>me</td>\n",
       "      <td>112.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>you</td>\n",
       "      <td>515.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word     gct  gfct     nct  nfct  gct-delta  gfct-delta\n",
       "2814   the  1154.0  97.0   980.0  98.0     -174.0         1.0\n",
       "2823  they   137.0  53.0    43.0  19.0      -94.0       -34.0\n",
       "114     an   110.0  43.0    38.0  11.0      -72.0       -32.0\n",
       "1260    he   143.0  44.0    79.0  28.0      -64.0       -16.0\n",
       "3030   war    47.0  15.0     0.0   0.0      -47.0       -15.0\n",
       "3175  your   155.0  58.0   487.0  67.0      332.0         9.0\n",
       "1920    oh    35.0  15.0   394.0  44.0      359.0        29.0\n",
       "1839    my   119.0  50.0   489.0  84.0      370.0        34.0\n",
       "1719    me   112.0  39.0   589.0  82.0      477.0        43.0\n",
       "3172   you   515.0  85.0  1260.0  96.0      745.0        11.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "top/bot # variance of # words (once per file) *last column*\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>gct</th>\n",
       "      <th>gfct</th>\n",
       "      <th>nct</th>\n",
       "      <th>nfct</th>\n",
       "      <th>gct-delta</th>\n",
       "      <th>gfct-delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>they</td>\n",
       "      <td>137.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-94.0</td>\n",
       "      <td>-34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>an</td>\n",
       "      <td>110.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>from</td>\n",
       "      <td>65.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>-28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>all</td>\n",
       "      <td>151.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>-24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>will</td>\n",
       "      <td>44.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>just</td>\n",
       "      <td>46.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>cause</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>me</td>\n",
       "      <td>112.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>baby</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>love</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word    gct  gfct    nct  nfct  gct-delta  gfct-delta\n",
       "2823   they  137.0  53.0   43.0  19.0      -94.0       -34.0\n",
       "114      an  110.0  43.0   38.0  11.0      -72.0       -32.0\n",
       "1084   from   65.0  42.0   23.0  14.0      -42.0       -28.0\n",
       "88      all  151.0  67.0  127.0  43.0      -24.0       -24.0\n",
       "3102   will   44.0  33.0   42.0  14.0       -2.0       -19.0\n",
       "1472   just   46.0  25.0  206.0  62.0      160.0        37.0\n",
       "461   cause   13.0   8.0  114.0  45.0      101.0        37.0\n",
       "1719     me  112.0  39.0  589.0  82.0      477.0        43.0\n",
       "177    baby   12.0   4.0  261.0  51.0      249.0        47.0\n",
       "1656   love   14.0  10.0  331.0  61.0      317.0        51.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests, pandas, io, numpy, argparse, math\n",
    "from featureEngineering import *\n",
    "\n",
    "trainingMatrix1,yArr1,labels1,fnames1 = getGagaData(gtype=0)\n",
    "trainingMatrix2,yArr2,labels2,fnames2 = getGagaData(gtype=1)\n",
    "\n",
    "m1,c1 = countWords2(trainingMatrix1, labels1, fnames1)\n",
    "m2,c2 = countWords2(trainingMatrix2, labels2, fnames2)\n",
    "\n",
    "m = mergeCounts(m1,m2)\n",
    "\n",
    "m = m.sort_values('gct-delta')\n",
    "print ('\\ntop/bot # variance of # words *2nd to last column*')\n",
    "m = m.sort_values('gct-delta')\n",
    "display (pandas.concat([m.head(),m.tail()]))\n",
    "\n",
    "print ('\\ntop/bot # variance of # words (once per file) *last column*')\n",
    "m = m.sort_values('gfct-delta')\n",
    "display (pandas.concat([m.head(),m.tail()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the most frequent words aren not so great.   Taking the 2nd table may work.\n",
    "\n",
    "Or I can just use scikit-learn's feature magic and see what they do.  Let me see...  http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "Using sckit <B>VarianceThreshold</B> - feature reduction magic 3189 -> 326 with a 0.80 variance setting!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3189)\n",
      "('Variance .8', (100, 326))\n",
      "[u'1977', u'48', u'again', u'ain', u'all', u'alone', u'am', u'ammunition', u'amp', u'an', u'and', u'another', u'are', u'around', u'as', u'at', u'away', u'baby', u'back', u'bad', u'be', u'beat', u'becomes', u'been', u'before', u'believe', u'bells', u'better', u'big', u'billy', u'bit', u'black', u'bombs', u'boulevard', u'boys', u'brixton', u'bullets', u'burn', u'burning', u'but', u'by', u'cadillac', u'call', u'calling', u'came', u'can', u'car', u'casbah', u'cause', u'cheat', u'city', u'clash', u'clean', u'come', u'coming', u'cool', u'cos', u'could', u'crazy', u'crush', u'daddy', u'danced', u'day', u'days', u'dead', u'death', u'dem', u'did', u'didn', u'die', u'do', u'don', u'door', u'doors', u'down', u'drive', u'drug', u'eh', u'else', u'em', u'emotion', u'england', u'enough', u'ever', u'every', u'everybody', u'everything', u'face', u'fail', u'fall', u'far', u'fe', u'feet', u'fight', u'finger', u'fingerpop', u'for', u'fought', u'four', u'free', u'from', u'funk', u'game', u'garage', u'gear', u'get', u'gets', u'gimme', u'girl', u'girls', u'giving', u'glory', u'go', u'gonna', u'good', u'got', u'gotta', u'groovy', u'guitar', u'gun', u'guns', u'had', u'hate', u'have', u'he', u'hear', u'heart', u'heat', u'hell', u'her', u'here', u'hey', u'him', u'his', u'hit', u'hitsville', u'home', u'horsemen', u'hours', u'how', u'hurts', u'if', u'in', u'inna', u'is', u'it', u'jah', u'jail', u'jazz', u'jerk', u'jimmy', u'jump', u'just', u'king', u'knock', u'know', u'knows', u'law', u'let', u'life', u'lightning', u'lights', u'like', u'little', u'live', u'll', u'london', u'long', u'look', u'looking', u'lot', u'love', u'loved', u'lover', u'make', u'mamma', u'man', u'me', u'money', u'more', u'move', u'movers', u'murdered', u'must', u'my', u'name', u'narrow', u'need', u'needs', u'never', u'new', u'night', u'no', u'not', u'now', u'of', u'off', u'oh', u'old', u'on', u'one', u'only', u'or', u'out', u'over', u'papa', u'people', u'pirate', u'place', u'play', u'point', u'points', u'police', u'put', u'quot', u'radio', u're', u'ready', u'really', u'red', u'revolution', u'rhythm', u'right', u'rights', u'rock', u'rockers', u'rode', u'roll', u'rudie', u'run', u'running', u'said', u'san', u'satellite', u'saw', u'say', u'see', u'shadow', u'shakers', u'shame', u'shareef', u'she', u'shop', u'so', u'some', u'somebody', u'sound', u'spanish', u'stabbin', u'stagger', u'stand', u'standing', u'start', u'stop', u'story', u'straight', u'street', u'strike', u'sun', u'take', u'talk', u'tell', u'that', u'the', u'their', u'them', u'then', u'there', u'these', u'they', u'thieves', u'thing', u'things', u'think', u'this', u'those', u'through', u'till', u'time', u'times', u'to', u'tommy', u'tonight', u'too', u'town', u'train', u'tune', u'under', u'up', u'us', u'use', u've', u'version', u'waiting', u'wall', u'wanna', u'want', u'war', u'was', u'way', u'we', u'well', u'went', u'were', u'what', u'whatever', u'when', u'where', u'who', u'why', u'wild', u'will', u'with', u'without', u'won', u'work', u'working', u'world', u'would', u'wrong', u'ya', u'yeah', u'yes', u'york', u'you', u'your']\n"
     ]
    }
   ],
   "source": [
    "t1 = numpy.array(trainingMatrix1)\n",
    "\n",
    "# VarianceThreadhold\n",
    "print (t1.shape)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "model = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "m = model.fit_transform(t1)\n",
    "print ('Variance .8',m.shape)\n",
    "\n",
    "picklist = model.get_support(True)\n",
    "pickwords = [labels1[p] for p in picklist]\n",
    "print (pickwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit <B>SelectKBest</B> (top 50) same magic goes from 3189 features to top 50 !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3189)\n",
      "('KBest', (100, 50))\n",
      "[u'workers', u'working', u'works', u'world', u'worried', u'worry', u'worse', u'worthy', u'wot', u'would', u'wouldn', u'wreck', u'writ', u'write', u'written', u'wrong', u'ya', u'yabbos', u'yankee', u'yard', u'yeah', u'year', u'years', u'yeh', u'yellow', u'yellowy', u'yer', u'yes', u'yesterday', u'yet', u'yo', u'yob', u'york', u'you', u'young', u'younger', u'your', u'yours', u'yourself', u'youth', u'zealot', u'zed', u'zee', u'zion', u'zombies', u'zone', u'zoo', u'zooming', u'zooms', u'zydeco']\n"
     ]
    }
   ],
   "source": [
    "# SelectKBest(50)\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X, y = t1, yArr1\n",
    "print(X.shape)\n",
    "model = SelectKBest(chi2, k=50)\n",
    "X_new = model.fit_transform(X, y)   # need to keep labels\n",
    "print('KBest',X_new.shape)\n",
    "\n",
    "picklist = model.get_support(True)\n",
    "pickwords = [labels1[p] for p in picklist]\n",
    "print (pickwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
