{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to TensorFlow\n",
    "\n",
    "Note I am reading the book as my guide -- https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291 - quite a good book so far.\n",
    "\n",
    "The goal here will be to explore building similar tools as I've done before:  \n",
    "- Basic linear regression solver\n",
    "- Basic logistic regression solver\n",
    "- Gradient Descent solver\n",
    "\n",
    "Then to move onto the next major area: Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow requires you to wrap your variables and methods into their own custom structures.  A few caveats:  \n",
    "- TF wants stuff in a different row/matrix orientation than SciKit.  Everyone wants it different which is annoying (but just requires a transpose so not a huge deal)\n",
    "- TF seems hard to debug since you wrap stuff and evaluate later in a graph\n",
    "- TF graph gui stuff looks slick\n",
    "- Its not so intuitive as a Py declarative programmer\n",
    "\n",
    "Code that does a basic BGD Logistic Regression Solver -- note TF isn't installed then it won't work   \n",
    "(Just pip install tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dbe3c15aa8d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_epochs = 400\n",
    "learning_rate = 0.01\n",
    "\n",
    "xs = np.array([[10,1],[11,2],[1,6]])   # dummy sample \n",
    "ys = np.array([[1],[1],[0]])           # dummy results\n",
    "\n",
    "X = tf.constant(xs, dtype=tf.float32, name='X')   # wrap in TF vanilla consts\n",
    "y = tf.constant(ys, dtype=tf.float32, name='y')   \n",
    "\n",
    "theta = tf.Variable(tf.constant([[0.1],[0.1]]), name='theta')  # TF \"variable\"\n",
    "y_pred = tf.sigmoid(tf.matmul(X, theta, name='predictions'))  # wrap in TF sigmoid\n",
    "\n",
    "with tf.name_scope(\"loss\"):                # named scope (for graph imagery gropuing)\n",
    "    error = y_pred - y                     # error used in next scope\n",
    "    ll = tf.reduce_mean(tf.losses.log_loss(y,y_pred), name='log_loss')  # std log_loss function, not used?\n",
    "\n",
    "with tf.name_scope(\"gradients\"):\n",
    "    gradients = 2.0/len(ys) * tf.matmul(tf.transpose(X), error)         # std partial deriv/gradient formula \n",
    "    training_op = tf.assign(theta, theta - learning_rate * gradients)   # \"training_op\" is called later\n",
    "\n",
    "init = tf.global_variables_initializer()   # boilerplate init\n",
    "\n",
    "with tf.Session() as sess:                 # this is where the TF stuff actually runs\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):          # GD loop\n",
    "        sess.run(training_op)              # each loop calls \"training_op\" again which assigns the theta\n",
    "    best_theta = theta.eval()              # fetch theta array\n",
    "    print(best_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "I was curious when GPU are faster.  I found a few things initially.\n",
    "\n",
    "- CPU without MMX is slow (no vector operations on matrix and BGD)\n",
    "- CPU w/ MMX is faster (should measure if I can disable MMX somehow)\n",
    "- GPU is not always faster (on basic MNIST NN image example it was ~8% faster)\n",
    "\n",
    "To setup GPU:\n",
    "- Suggest setup a virtualenv of conda env, so you keep the env clean if you want to compare GPU vs non-GPU\n",
    "- Install TensorFlow (tensorflow-gpu which includes tensorflow - 1.8 in my case)\n",
    "- Install CUDA drivers (9.0 in my case, its compiled for only this version in Windows)\n",
    "- Install Cuda NN DLL (7.1 - just copy DLL after installed to somewhere in path)\n",
    "\n",
    "Note my initial installs were screwed up.  I reinstalled tensorflow after installing CUDA and now its working.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
