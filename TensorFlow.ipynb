{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to TensorFlow\n",
    "\n",
    "Note I am reading the book as my guide -- https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291 - quite a good book so far.\n",
    "\n",
    "The goal here will be to explore building similar tools as I've done before:  \n",
    "- Basic linear regression solver\n",
    "- Basic logistic regression solver\n",
    "- Gradient Descent solver\n",
    "\n",
    "Then to move onto the next major area: Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow requires you to wrap your variables and methods into their own custom structures.  A few caveats:  \n",
    "- TF wants stuff in a different row/matrix orientation than SciKit.  Everyone wants it different which is annoying (but just requires a transpose so not a huge deal)\n",
    "- TF seems hard to debug since you wrap stuff and evaluate later in a graph\n",
    "- TF graph gui stuff looks slick\n",
    "- Its not so intuitive as a Py declarative programmer\n",
    "\n",
    "Code that does a basic BGD Logistic Regression Solver -- note TF isn't installed then it won't work   \n",
    "(Just pip install tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta computed:  ['0.6762', '-0.8275']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import myutils\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_epochs = 400\n",
    "learning_rate = 0.01\n",
    "\n",
    "xs = np.array([[10,1],[11,2],[1,6]])   # dummy sample  (high, high, low)\n",
    "ys = np.array([[1],[1],[0]])           # dummy results (correlates high count as positive, so 1,1,0 results)\n",
    "\n",
    "X = tf.constant(xs, dtype=tf.float32, name='X')   # wrap in TF vanilla consts\n",
    "y = tf.constant(ys, dtype=tf.float32, name='y')   \n",
    "\n",
    "theta = tf.Variable(tf.constant([[0.1],[0.1]]), name='theta')  # TF \"variable\"\n",
    "y_pred = tf.sigmoid(tf.matmul(X, theta, name='predictions'))  # wrap in TF sigmoid\n",
    "\n",
    "with tf.name_scope(\"loss\"):                # named scope (for graph imagery gropuing)\n",
    "    error = y_pred - y                     # error used in next scope\n",
    "    ll = tf.reduce_mean(tf.losses.log_loss(y,y_pred), name='log_loss')  # std log_loss function, not used?\n",
    "\n",
    "with tf.name_scope(\"gradients\"):\n",
    "    gradients = 2.0/len(ys) * tf.matmul(tf.transpose(X), error)         # std partial deriv/gradient formula \n",
    "    training_op = tf.assign(theta, theta - learning_rate * gradients)   # \"training_op\" is called later\n",
    "\n",
    "init = tf.global_variables_initializer()   # boilerplate init\n",
    "\n",
    "with tf.Session() as sess:                 # this is where the TF stuff actually runs\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):          # GD loop\n",
    "        sess.run(training_op)              # each loop calls \"training_op\" again which assigns the theta\n",
    "    best_theta = theta.eval()              # fetch theta array\n",
    "    print('theta computed: ', myutils.gf(best_theta))            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-doing the Lady Gaga Classifier\n",
    "\n",
    "Here we go, adapted to solve our favorite dummy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gaga test set 175 docs\n",
      "WARNING:root:non gaga test set 128 docs\n",
      "WARNING:root:feature reduce KBest: (213, 4000)\n",
      "WARNING:root:KBest 500 applied (213, 500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Log_Loss 0.59850425\n",
      "Epoch 500 Log_Loss 0.4192724\n",
      "Epoch 1000 Log_Loss 0.34695074\n",
      "Epoch 1500 Log_Loss 0.30071217\n",
      "Epoch 2000 Log_Loss 0.26779503\n",
      "['-0.0209', '-0.0039', '-0.0069', '-0.0125', '-0.0018', '-0.0244', '0.0235', '0.0125', '0.0118', '-0.1697', '0.0118', '-0.0051', '0.0354', '0.0138', '0.0438', '-0.0172', '0.0424', '-0.0088', '0.0309', '0.0315', '...']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1dd2fda36424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mtestResRound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestRes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtestDiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestResRound\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'raw results %s '\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestRes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'sig results %s'\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmyutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestRes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'0|1 results %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestRes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorExamples as te\n",
    "import pandas\n",
    "from gdsolvers import sigmoid\n",
    "import logging as log\n",
    "\n",
    "tf.reset_default_graph()\n",
    "n_epochs = 2500\n",
    "learning_rate = 0.01\n",
    "\n",
    "X,y,theta,y_pred,features,rfeatures,testMatrix,testY = te.getGagaTfFormat()\n",
    "m = len(testMatrix[0])\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    error = y_pred - y  # vs ll ?\n",
    "    ll = tf.reduce_mean(tf.losses.log_loss(y,y_pred), name='log_loss')  # -log(x) or -log(1-x) ....\n",
    "\n",
    "with tf.name_scope(\"gradients\"):\n",
    "    gradients = 2.0/m * tf.matmul(tf.transpose(X), error)  # vs ll vs error \n",
    "    training_op = tf.assign(theta, theta - learning_rate * gradients)   # what is this\n",
    "\n",
    "ll_summary = tf.summary.scalar('log_loss',ll)\n",
    "file_writer = tf.summary.FileWriter(myutils.getLogDir(),tf.get_default_graph())\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if (epoch % 500 == 0):\n",
    "            print('Epoch %s Log_Loss %s'%(epoch, ll.eval()))\n",
    "            summary_str = ll_summary.eval()  # bug\n",
    "            step = epoch\n",
    "            file_writer.add_summary(summary_str, step)\n",
    "        sess.run(training_op)   # whats an opp\n",
    "    best_theta = theta.eval()\n",
    "print(myutils.gf(best_theta))   # scores should be similar to sckit and grad5 solver\n",
    "file_writer.close()\n",
    "\n",
    "# reduce test set similarly (note below works because we know full set of train+test features ahead of time)    \n",
    "df = pandas.DataFrame(testMatrix, columns=features)   # new df w/ column names\n",
    "X = df[rfeatures].as_matrix()                # filter out only rfeatures\n",
    "\n",
    "testRes = np.dot(X, best_theta)\n",
    "testResRound = [round(sigmoid(x),0) for x in testRes]\n",
    "testDiffs = np.array(testResRound) - np.array(testY)\n",
    "log.warn ('raw results %s '%(myutils.gf(testRes)))\n",
    "log.warn ('sig results %s'% (myutils.gf([sigmoid(x) for x in testRes])))\n",
    "log.warn ('0|1 results %s'%([round(sigmoid(x),0) for x in testRes]))\n",
    "log.warn (testDiffs)\n",
    "log.error ('mymodel errors: %s / %s = %f'%(sum([abs(x) for x in testDiffs]),len(testY),sum([abs(x) for x in testDiffs])/len(testY)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "I was curious when GPU are faster.  I found a few things initially.\n",
    "\n",
    "- CPU without MMX is slow (no vector operations on matrix and BGD)\n",
    "- CPU w/ MMX is faster (should measure if I can disable MMX somehow)\n",
    "- GPU is not always faster (on basic MNIST NN image example it was ~8% faster)\n",
    "\n",
    "To setup GPU:\n",
    "- Suggest setup a virtualenv of conda env, so you keep the env clean if you want to compare GPU vs non-GPU\n",
    "- Install TensorFlow (tensorflow-gpu which includes tensorflow - 1.8 in my case)\n",
    "- Install CUDA drivers (9.0 in my case, its compiled for only this version in Windows)\n",
    "- Install Cuda NN DLL (7.1 - just copy DLL after installed to somewhere in path)\n",
    "\n",
    "Note my initial installs were screwed up.  I reinstalled tensorflow after installing CUDA and now its working.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
