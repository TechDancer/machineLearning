{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PyTorch\n",
    "\n",
    "After using TensorFlow and Scikit-Learn, I am finding all the frameworks probably have a similar feel.   The key things to learn:\n",
    "\n",
    "- How the framework wants its arrays -- numpy, custom, pandas ?\n",
    "- How to use the frameworks wrappers (if there are any)\n",
    "- Row or column orientation ?   Training examples stacked rows or columns\n",
    "- One-hot encoded Y-expected results ?\n",
    "\n",
    "PyTorch has its own set of wrappers, so you have to convert from ndarray<->torch structures.  Everything in Torch should be in Tensor structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9,  10,  11],\n",
      "        [ 12,  13,  14]], dtype=torch.int32) \n",
      " tensor([[  9,  10,  11],\n",
      "        [ 12,  13,  14]], dtype=torch.int32) \n",
      " tensor([[ 1,  1,  1],\n",
      "        [ 1,  1,  1]], dtype=torch.uint8)\n",
      "[[ 9 10 11]\n",
      " [12 13 14]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "torchTensorA = torch.from_numpy(numpy.array([[1,2,3],[4,5,6]]))\n",
    "torchTensorB = torch.from_numpy(numpy.array([8,8,8]))\n",
    "\n",
    "ttC1 = torchTensorA + torchTensorB\n",
    "ttC2 = torch.add(torchTensorA,torchTensorB)\n",
    "\n",
    "print (ttC1, '\\n', ttC2, '\\n', ttC1==ttC2)\n",
    "\n",
    "np1 = ttC1.numpy()\n",
    "print (np1, type(np1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda? False\n"
     ]
    }
   ],
   "source": [
    "# Torch has CUDA GPU support.\n",
    "print ('cuda?', torch.cuda.is_available())\n",
    "\n",
    "# Using it isn't too hard but TF seems easier:  https://pytorch.org/docs/stable/notes/cuda.html\n",
    "cuda = torch.device(\"cuda:0\") \n",
    "if (torch.cuda.is_available()):\n",
    "    x = torch.empty((8, 42), device=cuda)  # if u have a GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logistic Regression\n",
    "\n",
    "Yet another Logistic Regression Example !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gaga test set 100 docs\n",
      "WARNING:root:non gaga test set 100 docs\n",
      "C:\\Users\\dougfoo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "WARNING:root:loop 0, 55.07405853\n",
      "WARNING:root:loop 5000, 0.00000082\n",
      "WARNING:root:loop 10000, 0.00000021\n",
      "WARNING:root:loop 15000, 0.00000009\n",
      "WARNING:root:loop 20000, 0.00000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete  3.308371532284582e-08\n",
      "test validation phase\n",
      "total correct/tests 47 60\n",
      "correct % = 78.33\n"
     ]
    }
   ],
   "source": [
    "import torch, os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from myutils import *\n",
    "    \n",
    "# boilerplate setup my data\n",
    "data, yarr, features, fnames = getGagaData(maxrows=200, maxfeatures=2000, gtype=None, stopwords='english')\n",
    "xMatrix = shuffle(data, random_state=0)\n",
    "yArr = shuffle(np.array(yarr).reshape(-1,1), random_state=0)\n",
    "partition = int(.70*len(yArr))\n",
    "trainingX = xMatrix[:partition]\n",
    "trainingY = yArr[:partition]\n",
    "testX = xMatrix[partition:]\n",
    "testY = yArr[partition:]\n",
    "\n",
    "# Create random input and output data\n",
    "dtype = torch.float\n",
    "x = torch.tensor(trainingX, dtype=dtype)\n",
    "y = torch.tensor(trainingY, dtype=dtype)  \n",
    "testx = torch.tensor(testX, dtype=dtype)\n",
    "testy = torch.tensor(testY, dtype=dtype)  \n",
    "\n",
    "# Randomly initialize weights\n",
    "torch.manual_seed(0)\n",
    "w1 = torch.randn(len(features),1, dtype=dtype)\n",
    "\n",
    "# gradient descent\n",
    "learning_rate = 0.1\n",
    "for t in range(25000):\n",
    "    h = x.mm(w1)               # each feature * weight\n",
    "    y_pred = h.sigmoid()\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()  # item?\n",
    "    if (t % 5000 == 0):\n",
    "        log.warn('loop %d, %.8f'%(t, loss))\n",
    "\n",
    "    grad_w1 = x.t().mm(y_pred - y)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "\n",
    "print('training complete ', loss)\n",
    "print('test validation phase')\n",
    "\n",
    "h = testx.mm(w1)               # matrixMult or dot prod == same?\n",
    "y_pred = h.sigmoid().round()\n",
    "\n",
    "# y_pred = h.mm(w2)\n",
    "# y_pred_sig = y_pred.sigmoid()\n",
    "log.debug('ytest', pandas.DataFrame(testy.numpy()).head())\n",
    "log.debug('ypred', pandas.DataFrame(y_pred.numpy()).head())\n",
    "\n",
    "# Compute and print loss after rounding to 0/1's\n",
    "testDiffs = (y_pred - testy)\n",
    "p = pandas.DataFrame(testDiffs.numpy())\n",
    "log.debug('diffs', p.head())\n",
    "tests = len(p)\n",
    "correct = len(p[(p[0] == 0)])\n",
    "print('total correct/tests', correct, tests)\n",
    "print('correct % =', round((correct/tests)*100, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't bore everyone with the details above since its documented in prior notebooks.\n",
    "\n",
    "-----\n",
    "\n",
    "Onto the next step <B>Torch Neural Networks</B>, doing it the manual way first (manually calculating gradients/backprop).  However a basic intro to Neural Networks ..\n",
    "\n",
    "If Logistical Regression is like this:   \n",
    "![title](img/log-reg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neural network is adding 1 more layer like this (H<sub>1</sub> and H<sub>2</sub>) \n",
    "![title](img/log-reg-nn.png)\n",
    "\n",
    "----\n",
    "\n",
    "Thats not the clearest diagram, but I'll cleanup later.  The calibration of all the weights to minimize total error/cost is significantly more complicated with a multi-layer network.  You calculate the effect of w1 or w7 on the cost function, you are needing to calculate the partial derivatives and use the chain rule for dependent variables, eg:\n",
    "\n",
    "\\begin{align}\n",
    " \\frac{\\partial C }{\\partial w7} = \\frac{\\partial w7 }{\\partial h} * \\frac{\\partial h }{\\partial g} \\\\   \n",
    " \\frac{\\partial C }{\\partial w1} = \\frac{\\partial w1 }{\\partial H} * \\frac{\\partial H }{\\partial w7} ... \\\\   \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gaga test set 175 docs\n",
      "WARNING:root:non gaga test set 128 docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 212.7218780517578\n",
      "200 15.184028625488281\n",
      "400 13.640188217163086\n",
      "600 12.888275146484375\n",
      "800 13.629972457885742\n",
      "1000 13.452261924743652\n",
      "1200 13.511369705200195\n",
      "1400 14.130924224853516\n",
      "training complete  14.127641677856445\n",
      "predictions\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.000926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023020</td>\n",
       "      <td>0.978216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.952783</td>\n",
       "      <td>0.051358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.998770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.000305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.999103  0.000926\n",
       "1  0.023020  0.978216\n",
       "2  0.952783  0.051358\n",
       "3  0.001234  0.998770\n",
       "4  0.999720  0.000305"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct/tests 64 92\n",
      "correct % = 69.57\n",
      "hidden nodes 20\n"
     ]
    }
   ],
   "source": [
    "# example using some basic bits of PyTorch creates a 500-20-2 neural net for the Gaga text classifier\n",
    "# and manually calculates the backprop gradients\n",
    "\n",
    "dtype, device = torch.float,torch.device(\"cpu\")\n",
    "\n",
    "F,H, D_out = 500,20, 2 # features, hiddennodes, output nodes\n",
    "D_in = F\n",
    "\n",
    "# standard training/test data setup\n",
    "data, yarr, features, fnames = getGagaData(maxrows=500, maxfeatures=F, gtype=None, stopwords='english')\n",
    "xMatrix = shuffle(data, random_state=0)\n",
    "yArr = shuffle(yarr, random_state=0)\n",
    "\n",
    "partition = int(.70*len(yArr))\n",
    "trainingX = xMatrix[:partition]\n",
    "trainingY = yArr[:partition]\n",
    "testX = xMatrix[partition:]\n",
    "testY = yArr[partition:]\n",
    "\n",
    "# add torch wrappers\n",
    "x = torch.tensor(trainingX, dtype=dtype)\n",
    "y = torch.tensor(pd.get_dummies(trainingY).values, dtype=dtype)  # onehot y's\n",
    "testx = torch.tensor(testX, dtype=dtype)\n",
    "testy = torch.tensor(pd.get_dummies(testY).values, dtype=dtype)  # onehot y's\n",
    "\n",
    "# Randomly initialize weights, repeatable w/ seed\n",
    "np.random.seed(0)\n",
    "w1 = torch.tensor(np.random.rand(D_in, H), dtype=dtype, requires_grad=False)\n",
    "w2 = torch.tensor(np.random.rand(H, D_out), dtype=dtype, requires_grad=False)\n",
    "\n",
    "#gradient descent\n",
    "learning_rate = 0.005\n",
    "for t in range(1500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)               # matrixMult or dot prod == same?\n",
    "    h_sig = h.sigmoid()        \n",
    "    y_pred = h_sig.mm(w2).sigmoid()\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum().item()  # item unwraps\n",
    "    if (t % 200 == 0):\n",
    "        print(t, loss)\n",
    "        if (loss < 0.0001):\n",
    "            break\n",
    "\n",
    "    # Manual backprop routines\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_sig.t().mm(grad_y_pred) \n",
    "    grad_h_sig = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_sig.clone()\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "print('training complete ',loss)\n",
    "\n",
    "h = testx.mm(w1)               # matrixMult or dot prod == same?\n",
    "h_sig = h.sigmoid()\n",
    "y_pred = h_sig.mm(w2).sigmoid()\n",
    "y_pred2 = torch.tensor(pd.get_dummies(y_pred.argmax(dim=1)).values, dtype=dtype)\n",
    "\n",
    "print('predictions\\n')\n",
    "display(pandas.DataFrame(y_pred.numpy()).head())\n",
    "\n",
    "# Compute and print loss after rounding to 0/1's\n",
    "testDiffs = (y_pred2 - testy)\n",
    "p = pandas.DataFrame(testDiffs.numpy())\n",
    "tests = len(p)\n",
    "correct = len(p[(p[0]==0) & (p[1]==0)])\n",
    "print('total correct/tests',correct,tests)\n",
    "print('correct % =', round((correct/tests)*100,2))\n",
    "print('hidden nodes %d'%H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gaga test set 175 docs\n",
      "WARNING:root:non gaga test set 128 docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 212.7218780517578\n",
      "200 170.50454711914062\n",
      "400 70.86431884765625\n",
      "600 48.30420684814453\n",
      "800 31.754566192626953\n",
      "1000 23.829137802124023\n",
      "1200 19.80072784423828\n",
      "1400 17.226394653320312\n",
      "training complete  tensor(16.4909)\n",
      "ypred\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.954171</td>\n",
       "      <td>0.043588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.995483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.977425</td>\n",
       "      <td>0.024113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005237</td>\n",
       "      <td>0.994935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999485</td>\n",
       "      <td>0.000576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.954171  0.043588\n",
       "1  0.003371  0.995483\n",
       "2  0.977425  0.024113\n",
       "3  0.005237  0.994935\n",
       "4  0.999485  0.000576"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct/tests 70 92\n",
      "correct % = 76.09\n",
      "hidden nodes 20\n"
     ]
    }
   ],
   "source": [
    "## Same as above, but using PyTorch autodiff to do backprop and calculate gradients automatically using a dependency graph\n",
    "## notice results are sligthly different which is odd (?)\n",
    "\n",
    "dtype, device = torch.float, torch.device(\"cpu\")\n",
    "F, H = 500, 20   # features\n",
    "D_in, D_out = F, 2  # 100 hidden nodes, 2 output nodes\n",
    "\n",
    "#boilerplate stuff preparing the training/test data\n",
    "data, yarr, features, fnames = getGagaData(maxrows=500, maxfeatures=F, gtype=None, stopwords='english')\n",
    "xMatrix = shuffle(data, random_state=0)\n",
    "yArr = shuffle(yarr, random_state=0)\n",
    "\n",
    "partition = int(.70*len(yArr))\n",
    "trainingX = xMatrix[:partition]\n",
    "trainingY = yArr[:partition]\n",
    "testX = xMatrix[partition:]\n",
    "testY = yArr[partition:]\n",
    "\n",
    "# add torch wrappers\n",
    "x = torch.tensor(trainingX, dtype=dtype)\n",
    "y = torch.tensor(pd.get_dummies(trainingY).values, dtype=dtype)  # onehot y's\n",
    "testx = torch.tensor(testX, dtype=dtype)\n",
    "testy = torch.tensor(pd.get_dummies(testY).values, dtype=dtype)  # onehot y's\n",
    "\n",
    "# Randomly initialize weights, repeatable w/ seed, put w1/w2 on graph\n",
    "np.random.seed(0)\n",
    "w1 = torch.tensor(np.random.rand(D_in, H), dtype=dtype, requires_grad=True)   # requires_grad = True puts on graph\n",
    "w2 = torch.tensor(np.random.rand(H, D_out), dtype=dtype, requires_grad=True)  # requires_grad = True puts on graph\n",
    "\n",
    "#gradient descent using autograd\n",
    "learning_rate = 0.005\n",
    "for t in range(1500):\n",
    "    # Forward pass:2compute predicted y\n",
    "    h = x.mm(w1).sigmoid()               # matrixMult or dot prod == same?\n",
    "    y_pred = h.mm(w2).sigmoid()\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()  # item unwraps\n",
    "    if (t % 200 == 0):\n",
    "        print(t, loss.item())\n",
    "        if (loss < 0.0001):\n",
    "            break\n",
    "\n",
    "    # autograd backprop ** only real difference\n",
    "    loss.backward()  # goes thru graph and calculates all .grad values magically !!\n",
    "    with torch.no_grad():  # take off graph for this step\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "\n",
    "print('training complete ', loss)\n",
    "\n",
    "h = testx.mm(w1).sigmoid()               # matrixMult or dot prod == same?\n",
    "y_pred = h.mm(w2).sigmoid()\n",
    "y_pred2 = torch.tensor(pd.get_dummies(y_pred.argmax(dim=1)).values, dtype=dtype)\n",
    "\n",
    "print('ypred')\n",
    "display(pandas.DataFrame(y_pred.detach().numpy()).head())\n",
    "testDiffs = (y_pred2 - testy)\n",
    "p = pandas.DataFrame(testDiffs.numpy())\n",
    "tests = len(p)\n",
    "correct = len(p[(p[0] == 0) & (p[1] == 0)])\n",
    "print('total correct/tests', correct, tests)\n",
    "print('correct % =', round((correct/tests)*100, 2))\n",
    "print('hidden nodes %d' % H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only real change is setting autograd=True on the w1,w2 values then removing the manual backprop steps and just extracting the .grad values.  Adding w1,w2 makes all dependent variables (like X) automatically added to the graph.  Once on graph you can't manipulate those variables until you go offgraph (headache yes).  Having it on graph does the partial derivs and calculates all gradients in the graph automatically which is amazing/magical and just plain conveninent.\n",
    "\n",
    "----\n",
    "\n",
    "Above doesn't show great results 70% -- but with only 1500 iterations its ok.  How neural network back propagation works needs some focus if you want to understand it.  It is complex enough that a few videos are suggested (The series by 3Brown1Blue - https://www.youtube.com/watch?v=Ilg3gGewQ5U&feature=youtu.be highly recommended).   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# PyTorch Neural Network\n",
    "\n",
    "PyTorch has its own framework in torch.nn -- Module and its subclasses like Linear, Conv#d, RNN, etc.\n",
    "\n",
    "First define the Network: 500 input - 20 hidden - 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GagaNet(\n",
      "  (inp): Linear(in_features=500, out_features=20, bias=False)\n",
      "  (hid): Linear(in_features=20, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as tfun\n",
    "\n",
    "# extends Net w/ fc internal model 500:20:1 model, using sigmoid activations\n",
    "class GagaNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GagaNet, self).__init__()\n",
    "        self.inp = nn.Linear(500, 20, bias=False)   # fully connected linear layers - 500 input to 20 output\n",
    "        self.hid = nn.Linear(20, 1, bias=False)     # fc layer for hidden 20 input nodes, 1 output\n",
    "\n",
    "    # forward prop -wrap both steps in sigmoid\n",
    "    def forward(self, x):\n",
    "        x = tfun.sigmoid(self.inp(x))  # oddly works better w/o this sigmoid\n",
    "        x = tfun.sigmoid(self.hid(x))  \n",
    "        return x\n",
    "\n",
    "net = GagaNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next setup the training - same boring stuff, but wrapped in torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:gaga test set 175 docs\n",
      "WARNING:root:non gaga test set 128 docs\n"
     ]
    }
   ],
   "source": [
    "# get test data\n",
    "data, yarr, features, fnames = getGagaData(maxrows=500, maxfeatures=500, gtype=None, stopwords='english')\n",
    "\n",
    "xMatrix = shuffle(data, random_state=0)\n",
    "yArr = shuffle(yarr, random_state=0)\n",
    "partition = int(.70*len(yArr))\n",
    "trainingX = xMatrix[:partition]\n",
    "trainingY = yArr[:partition]\n",
    "testX = xMatrix[partition:]\n",
    "testY = yArr[partition:]\n",
    "\n",
    "input = torch.tensor(xMatrix, dtype=torch.float)             # m x 500\n",
    "target = torch.tensor(yArr, dtype=torch.float).view(-1,1)    # 1 x m\n",
    "test_input = torch.tensor(testX, dtype=torch.float)          # m x 500\n",
    "test_target = torch.tensor(testY, dtype=torch.float).view(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean loss: tensor(0.1015)\n",
      "weights spot check tensor([ 0.0064, -0.0239, -0.0181,  ...,  0.0434, -0.0041,  0.0255])\n",
      "Epoch 500, mean loss: tensor(0.1004)\n",
      "weights spot check tensor([ 0.0060, -0.0246, -0.0188,  ...,  0.0434, -0.0036,  0.0255])\n",
      "Epoch 1000, mean loss: tensor(1.00000e-02 *\n",
      "       9.9337)\n",
      "weights spot check tensor([ 0.0056, -0.0253, -0.0194,  ...,  0.0434, -0.0032,  0.0255])\n",
      "Epoch 1500, mean loss: tensor(1.00000e-02 *\n",
      "       9.8291)\n",
      "weights spot check tensor([ 0.0051, -0.0260, -0.0200,  ...,  0.0434, -0.0028,  0.0256])\n",
      "Epoch 2000, mean loss: tensor(1.00000e-02 *\n",
      "       9.7262)\n",
      "weights spot check tensor([ 0.0047, -0.0266, -0.0206,  ...,  0.0434, -0.0024,  0.0256])\n",
      "-------------\n",
      "training done\n",
      "-----------\n",
      "\n",
      "predict sigmoid tensor([[ 0.4157,  0.9996,  0.0714,  ...,  0.4837,  0.6227,  0.6707]])\n",
      "predict rounded tensor([[ 0.,  1.,  0.,  ...,  0.,  1.,  1.]])\n",
      "expected output tensor([[ 1.,  1.,  0.,  ...,  1.,  1.,  1.]])\n",
      "err 10 / total 92 = 0.891304 accuracy \n"
     ]
    }
   ],
   "source": [
    "# run the NN training\n",
    "import torch.optim as optim\n",
    "torch.set_printoptions(threshold=10)   # limit tensor printing\n",
    "\n",
    "criterion = nn.MSELoss()   # loss function mean-square-error\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01) # stochastic grad descent\n",
    "\n",
    "# in your training loop:\n",
    "for epoch in range(2500):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(input)     # forward prop __call__ \n",
    "    loss = criterion(output, target)   # loss optimization    \n",
    "    loss.backward()         # auto backward prop\n",
    "    optimizer.step()        # does the update of applying gradients to weights\n",
    "    if (epoch % 500 == 0):\n",
    "        print('Epoch %s, mean loss: %s' % (epoch, loss))\n",
    "        print('weights spot check',net.inp.weight[0])\n",
    "print('-------------\\ntraining done\\n-----------\\n')\n",
    "\n",
    "# now do testing\n",
    "test_res = net.forward(test_input)\n",
    "test_res_round = test_res.round()\n",
    "test_diff = test_res_round - test_target\n",
    "print ('predict sigmoid', test_res.view(1,-1))  \n",
    "print ('predict rounded', test_res_round.view(1,-1))  \n",
    "print ('expected output', test_target.view(1,-1))  \n",
    "print ('err %d / total %d = %2f accuracy '%(test_diff.abs().sum().item(), len(test_diff),(len(test_diff)- test_diff.abs().sum().item()) / (len(test_diff))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy -- similar to TF, its all about figuring out the inputs, wrapping them, then making the macro calls to train your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
