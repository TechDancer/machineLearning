{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PyTorch\n",
    "\n",
    "After using TensorFlow and Scikit-Learn, I am finding all the frameworks probably have a similar feel.   The key things to learn:\n",
    "\n",
    "- How the framework wants its arrays -- numpy, custom, pandas ?\n",
    "- How to use the frameworks wrappers (if there are any)\n",
    "- Row or column orientation ?   Training examples stacked rows or columns\n",
    "- One-hot encoded Y-expected results ?\n",
    "\n",
    "PyTorch has its own set of wrappers, so you have to convert from ndarray<->torch structures.  Everything in Torch should be in Tensor structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9,  10,  11],\n",
      "        [ 12,  13,  14]], dtype=torch.int32) \n",
      " tensor([[  9,  10,  11],\n",
      "        [ 12,  13,  14]], dtype=torch.int32) \n",
      " tensor([[ 1,  1,  1],\n",
      "        [ 1,  1,  1]], dtype=torch.uint8)\n",
      "[[ 9 10 11]\n",
      " [12 13 14]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "torchTensorA = torch.from_numpy(numpy.array([[1,2,3],[4,5,6]]))\n",
    "torchTensorB = torch.from_numpy(numpy.array([8,8,8]))\n",
    "\n",
    "ttC1 = torchTensorA + torchTensorB\n",
    "ttC2 = torch.add(torchTensorA,torchTensorB)\n",
    "\n",
    "print (ttC1, '\\n', ttC2, '\\n', ttC1==ttC2)\n",
    "\n",
    "np1 = ttC1.numpy()\n",
    "print (np1, type(np1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda? False\n"
     ]
    }
   ],
   "source": [
    "# Torch has CUDA GPU support.\n",
    "print ('cuda?', torch.cuda.is_available())\n",
    "\n",
    "# Using it isn't too hard but TF seems easier:  https://pytorch.org/docs/stable/notes/cuda.html\n",
    "cuda = torch.device(\"cuda:0\") \n",
    "if (torch.cuda.is_available()):\n",
    "    x = torch.empty((8, 42), device=cuda)  # if u have a GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logistic Regression\n",
    "\n",
    "Yet another Logistic Regression Example !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch, os, sys\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from myutils import *\n",
    "    \n",
    "# boilerplate setup my data\n",
    "data, yarr, features, fnames = getGagaData(maxrows=200, maxfeatures=2000, gtype=None, stopwords='english')\n",
    "xMatrix = shuffle(data, random_state=0)\n",
    "yArr = shuffle(np.array(yarr).reshape(-1,1), random_state=0)\n",
    "partition = int(.70*len(yArr))\n",
    "trainingX = xMatrix[:partition]\n",
    "trainingY = yArr[:partition]\n",
    "testX = xMatrix[partition:]\n",
    "testY = yArr[partition:]\n",
    "\n",
    "# Create random input and output data\n",
    "dtype = torch.float\n",
    "x = torch.tensor(trainingX, dtype=dtype)\n",
    "y = torch.tensor(trainingY, dtype=dtype)  \n",
    "testx = torch.tensor(testX, dtype=dtype)\n",
    "testy = torch.tensor(testY, dtype=dtype)  \n",
    "\n",
    "# Randomly initialize weights\n",
    "torch.manual_seed(0)\n",
    "w1 = torch.randn(len(features),1, dtype=dtype)\n",
    "\n",
    "# gradient descent\n",
    "learning_rate = 0.1\n",
    "for t in range(25000):\n",
    "    h = x.mm(w1)               # each feature * weight\n",
    "    y_pred = h.sigmoid()\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()  # item?\n",
    "    if (t % 5000 == 0):\n",
    "        log.warn('loop %d, %.8f'%(t, loss))\n",
    "\n",
    "    grad_w1 = x.t().mm(y_pred - y)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "\n",
    "print('training complete ', loss)\n",
    "print('test validation phase')\n",
    "\n",
    "h = testx.mm(w1)               # matrixMult or dot prod == same?\n",
    "y_pred = h.sigmoid().round()\n",
    "\n",
    "# y_pred = h.mm(w2)\n",
    "# y_pred_sig = y_pred.sigmoid()\n",
    "log.debug('ytest', pandas.DataFrame(testy.numpy()).head())\n",
    "log.debug('ypred', pandas.DataFrame(y_pred.numpy()).head())\n",
    "\n",
    "# Compute and print loss after rounding to 0/1's\n",
    "testDiffs = (y_pred - testy)\n",
    "p = pandas.DataFrame(testDiffs.numpy())\n",
    "log.debug('diffs', p.head())\n",
    "tests = len(p)\n",
    "correct = len(p[(p[0] == 0)])\n",
    "print('total correct/tests', correct, tests)\n",
    "print('correct % =', round((correct/tests)*100, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't bore everyone with the details above since its documented in prior notebooks.\n",
    "\n",
    "Onto the next step <B>Torch Neural Networks</B>, doing it the manual way first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
