{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0ab9e6f88fb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "print('init')\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 10000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "print(sc)\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.csv(\"../nlp/stanfordSentimentTreebank/dictionary_sm.txt\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n",
    "\n",
    "df2 = spark.read.csv(\"../nlp/stanfordSentimentTreebank/sentiment_labels_sm.txt\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"../nlp/stanfordSentimentTreebank/dictionary_sm.txt\",\n",
    "                     format=\"csv\", sep=\"|\", inferSchema=\"true\", header=\"true\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.createOrReplaceGlobalTempView(\"sentences\")\n",
    "spark.sql(\"SELECT * FROM global_temp.sentences\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"../nlp/stanfordSentimentTreebank/dictionary_sm.txt\")\n",
    "parts = lines.map(lambda l: l.split(\"|\"))\n",
    "# Each line is converted to a tuple.\n",
    "wordid = parts.map(lambda p: (p[0].strip(), p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"word id\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaWord = spark.createDataFrame(wordid, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaWord.createOrReplaceTempView(\"word\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT * FROM word\")\n",
    "\n",
    "results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"saved_words2\")\n",
    "\n",
    "spark.sql(\"show databases\").show()\n",
    "spark.sql(\"show tables\").show()\n",
    "spark.sql(\"select * from saved_words2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"words.parquet\")\n",
    "parquetFile = spark.read.parquet(\"words.parquet\")\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "words = spark.sql(\"SELECT * FROM parquetFile \")\n",
    "words.show()\n",
    "words2 = spark.sql(\"SELECT * FROM parquetFile \")\n",
    "words2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import exp\n",
    "\n",
    "# df = spark.createDataFrame([(1, \"a\", 23.0), (3, \"B\", -23.0)], (\"x1\", \"x2\", \"x3\"))\n",
    "df_with_x4 = words2.withColumn(\"x4\", lit(0))\n",
    "#df_with_x4.show()\n",
    "\n",
    "\n",
    "df_with_x5 = words2.withColumn(\"x5\", words2.id %2 )\n",
    "df_with_x5.show()\n",
    "\n",
    "df_with_x5.write.mode(\"overwrite\").parquet(\"test.parquet\")\n",
    "\n",
    "df_with_x5.write.partitionBy('x5').mode(\"overwrite\").parquet(\"test.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "\n",
    "squaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n",
    "                                  .map(lambda i: Row(single=i, double=i ** 2)))\n",
    "squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "\n",
    "# Create another DataFrame in a new partition directory,\n",
    "# adding a new column and dropping an existing column\n",
    "cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "cubesDF.write.parquet(\"data/test_table/key=2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the partitioned table\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\n",
    "mergedDF.printSchema()\n",
    "mergedDF.show()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "result_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "print(distData.reduce(lambda a, b: a + b))\n",
    "print(distData.map(lambda a: a ** 2).reduce(lambda a,b: a+b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile = sc.textFile(\"../nlp/stanfordSentimentTreebank/dictionary*.txt\")\n",
    "distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "# conf = SparkConf()\n",
    "# conf.setMaster('spark://192.168.1.28:7077')\n",
    "# conf.setAppName('mynewapp2')\n",
    "# sc2 = SparkContext(conf=conf)\n",
    "# print (sc2)\n",
    "\n",
    "# def mod(x):\n",
    "#     import numpy as np\n",
    "#     return (x, np.mod(x, 2))\n",
    "# rdd = sc2.parallelize(range(100)).map(mod).take(10)\n",
    "# rdd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
