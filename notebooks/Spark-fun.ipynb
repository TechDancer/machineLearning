{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "<SparkContext master=local[*] appName=Pi>\n",
      "3.1436\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "print('init')\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 10000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "print(sc)\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1febb3fe148>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|             text|id|\n",
      "|                 !|0|\n",
      "|           ! '|22935|\n",
      "|          ! ''|18235|\n",
      "|       ! Alas|179257|\n",
      "|   ! Brilliant|22936|\n",
      "| ! Brilliant !|40532|\n",
      "|! Brilliant ! '|2...|\n",
      "|       ! C'mon|60624|\n",
      "|! Gollum 's ` per...|\n",
      "|               ! Oh |\n",
      "|  ! Romething|140882|\n",
      "|        ! Run|179259|\n",
      "|   ! The Movie|60625|\n",
      "|! The camera twir...|\n",
      "|! True Hollywood ...|\n",
      "|        ! Wow|179261|\n",
      "|     ! Zoom !|179262|\n",
      "|           !?|220445|\n",
      "|         !? '|220446|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+\n",
      "|         _c0|\n",
      "+------------+\n",
      "|id|sentiment|\n",
      "|       0|0.5|\n",
      "|       1|0.5|\n",
      "|   2|0.44444|\n",
      "|       3|0.5|\n",
      "|   4|0.42708|\n",
      "|     5|0.375|\n",
      "|   6|0.41667|\n",
      "|   7|0.54167|\n",
      "|   8|0.33333|\n",
      "|   9|0.45833|\n",
      "|  10|0.47222|\n",
      "|  11|0.59722|\n",
      "|  12|0.33333|\n",
      "|  13|0.93056|\n",
      "|  14|0.80556|\n",
      "|  15|0.81944|\n",
      "|  16|0.76389|\n",
      "|      17|0.5|\n",
      "|      18|0.5|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.csv(\"../nlp/stanfordSentimentTreebank/dictionary_sm.txt\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n",
    "\n",
    "df2 = spark.read.csv(\"../nlp/stanfordSentimentTreebank/sentiment_labels_sm.txt\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|    id|\n",
      "+--------------------+------+\n",
      "|                   !|     0|\n",
      "|                 ! '| 22935|\n",
      "|                ! ''| 18235|\n",
      "|              ! Alas|179257|\n",
      "|         ! Brilliant| 22936|\n",
      "|       ! Brilliant !| 40532|\n",
      "|     ! Brilliant ! '| 22937|\n",
      "|             ! C'mon| 60624|\n",
      "|! Gollum 's ` per...| 13402|\n",
      "|! Oh , look at th...|179258|\n",
      "|         ! Romething|140882|\n",
      "|               ! Run|179259|\n",
      "|         ! The Movie| 60625|\n",
      "|! The camera twir...|179260|\n",
      "|! True Hollywood ...|140883|\n",
      "|               ! Wow|179261|\n",
      "|            ! Zoom !|179262|\n",
      "|                  !?|220445|\n",
      "|                !? '|220446|\n",
      "|                   #| 60626|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"../nlp/stanfordSentimentTreebank/dictionary_sm.txt\",\n",
    "                     format=\"csv\", sep=\"|\", inferSchema=\"true\", header=\"true\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n",
      "+--------------------+------+\n",
      "|                text|    id|\n",
      "+--------------------+------+\n",
      "|                   !|     0|\n",
      "|                 ! '| 22935|\n",
      "|                ! ''| 18235|\n",
      "|              ! Alas|179257|\n",
      "|         ! Brilliant| 22936|\n",
      "|       ! Brilliant !| 40532|\n",
      "|     ! Brilliant ! '| 22937|\n",
      "|             ! C'mon| 60624|\n",
      "|! Gollum 's ` per...| 13402|\n",
      "|! Oh , look at th...|179258|\n",
      "|         ! Romething|140882|\n",
      "|               ! Run|179259|\n",
      "|         ! The Movie| 60625|\n",
      "|! The camera twir...|179260|\n",
      "|! True Hollywood ...|140883|\n",
      "|               ! Wow|179261|\n",
      "|            ! Zoom !|179262|\n",
      "|                  !?|220445|\n",
      "|                !? '|220446|\n",
      "|                   #| 60626|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.createOrReplaceGlobalTempView(\"sentences\")\n",
    "spark.sql(\"SELECT * FROM global_temp.sentences\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                word|    id|\n",
      "+--------------------+------+\n",
      "|                text|    id|\n",
      "|                   !|     0|\n",
      "|                 ! '| 22935|\n",
      "|                ! ''| 18235|\n",
      "|              ! Alas|179257|\n",
      "|         ! Brilliant| 22936|\n",
      "|       ! Brilliant !| 40532|\n",
      "|     ! Brilliant ! '| 22937|\n",
      "|             ! C'mon| 60624|\n",
      "|! Gollum 's ` per...| 13402|\n",
      "|! Oh , look at th...|179258|\n",
      "|         ! Romething|140882|\n",
      "|               ! Run|179259|\n",
      "|         ! The Movie| 60625|\n",
      "|! The camera twir...|179260|\n",
      "|! True Hollywood ...|140883|\n",
      "|               ! Wow|179261|\n",
      "|            ! Zoom !|179262|\n",
      "|                  !?|220445|\n",
      "|                !? '|220446|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"../nlp/stanfordSentimentTreebank/dictionary_sm.txt\")\n",
    "parts = lines.map(lambda l: l.split(\"|\"))\n",
    "# Each line is converted to a tuple.\n",
    "wordid = parts.map(lambda p: (p[0].strip(), p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"word id\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaWord = spark.createDataFrame(wordid, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaWord.createOrReplaceTempView(\"word\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT * FROM word\")\n",
    "\n",
    "results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n",
      "+--------+------------+-----------+\n",
      "|database|   tableName|isTemporary|\n",
      "+--------+------------+-----------+\n",
      "| default|saved_words2|      false|\n",
      "|        | parquetfile|       true|\n",
      "|        |        word|       true|\n",
      "+--------+------------+-----------+\n",
      "\n",
      "+--------------------+------+\n",
      "|                text|    id|\n",
      "+--------------------+------+\n",
      "|                   !|     0|\n",
      "|                 ! '| 22935|\n",
      "|                ! ''| 18235|\n",
      "|              ! Alas|179257|\n",
      "|         ! Brilliant| 22936|\n",
      "|       ! Brilliant !| 40532|\n",
      "|     ! Brilliant ! '| 22937|\n",
      "|             ! C'mon| 60624|\n",
      "|! Gollum 's ` per...| 13402|\n",
      "|! Oh , look at th...|179258|\n",
      "|         ! Romething|140882|\n",
      "|               ! Run|179259|\n",
      "|         ! The Movie| 60625|\n",
      "|! The camera twir...|179260|\n",
      "|! True Hollywood ...|140883|\n",
      "|               ! Wow|179261|\n",
      "|            ! Zoom !|179262|\n",
      "|                  !?|220445|\n",
      "|                !? '|220446|\n",
      "|                   #| 60626|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"saved_words2\")\n",
    "\n",
    "spark.sql(\"show databases\").show()\n",
    "spark.sql(\"show tables\").show()\n",
    "spark.sql(\"select * from saved_words2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|    id|\n",
      "+--------------------+------+\n",
      "|                   !|     0|\n",
      "|                 ! '| 22935|\n",
      "|                ! ''| 18235|\n",
      "|              ! Alas|179257|\n",
      "|         ! Brilliant| 22936|\n",
      "|       ! Brilliant !| 40532|\n",
      "|     ! Brilliant ! '| 22937|\n",
      "|             ! C'mon| 60624|\n",
      "|! Gollum 's ` per...| 13402|\n",
      "|! Oh , look at th...|179258|\n",
      "|         ! Romething|140882|\n",
      "|               ! Run|179259|\n",
      "|         ! The Movie| 60625|\n",
      "|! The camera twir...|179260|\n",
      "|! True Hollywood ...|140883|\n",
      "|               ! Wow|179261|\n",
      "|            ! Zoom !|179262|\n",
      "|                  !?|220445|\n",
      "|                !? '|220446|\n",
      "|                   #| 60626|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+------+\n",
      "|                text|    id|\n",
      "+--------------------+------+\n",
      "|                   !|     0|\n",
      "|                 ! '| 22935|\n",
      "|                ! ''| 18235|\n",
      "|              ! Alas|179257|\n",
      "|         ! Brilliant| 22936|\n",
      "|       ! Brilliant !| 40532|\n",
      "|     ! Brilliant ! '| 22937|\n",
      "|             ! C'mon| 60624|\n",
      "|! Gollum 's ` per...| 13402|\n",
      "|! Oh , look at th...|179258|\n",
      "|         ! Romething|140882|\n",
      "|               ! Run|179259|\n",
      "|         ! The Movie| 60625|\n",
      "|! The camera twir...|179260|\n",
      "|! True Hollywood ...|140883|\n",
      "|               ! Wow|179261|\n",
      "|            ! Zoom !|179262|\n",
      "|                  !?|220445|\n",
      "|                !? '|220446|\n",
      "|                   #| 60626|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"words.parquet\")\n",
    "parquetFile = spark.read.parquet(\"words.parquet\")\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "words = spark.sql(\"SELECT * FROM parquetFile \")\n",
    "words.show()\n",
    "words2 = spark.sql(\"SELECT * FROM parquetFile \")\n",
    "words2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+---+\n",
      "|                text|    id| x5|\n",
      "+--------------------+------+---+\n",
      "|                   !|     0|  0|\n",
      "|                 ! '| 22935|  1|\n",
      "|                ! ''| 18235|  1|\n",
      "|              ! Alas|179257|  1|\n",
      "|         ! Brilliant| 22936|  0|\n",
      "|       ! Brilliant !| 40532|  0|\n",
      "|     ! Brilliant ! '| 22937|  1|\n",
      "|             ! C'mon| 60624|  0|\n",
      "|! Gollum 's ` per...| 13402|  0|\n",
      "|! Oh , look at th...|179258|  0|\n",
      "|         ! Romething|140882|  0|\n",
      "|               ! Run|179259|  1|\n",
      "|         ! The Movie| 60625|  1|\n",
      "|! The camera twir...|179260|  0|\n",
      "|! True Hollywood ...|140883|  1|\n",
      "|               ! Wow|179261|  1|\n",
      "|            ! Zoom !|179262|  0|\n",
      "|                  !?|220445|  1|\n",
      "|                !? '|220446|  0|\n",
      "|                   #| 60626|  0|\n",
      "+--------------------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import exp\n",
    "\n",
    "# df = spark.createDataFrame([(1, \"a\", 23.0), (3, \"B\", -23.0)], (\"x1\", \"x2\", \"x3\"))\n",
    "df_with_x4 = words2.withColumn(\"x4\", lit(0))\n",
    "#df_with_x4.show()\n",
    "\n",
    "\n",
    "df_with_x5 = words2.withColumn(\"x5\", words2.id %2 )\n",
    "df_with_x5.show()\n",
    "\n",
    "df_with_x5.write.mode(\"overwrite\").parquet(\"test.parquet\")\n",
    "\n",
    "df_with_x5.write.partitionBy('x5').mode(\"overwrite\").parquet(\"test.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "\n",
    "squaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n",
    "                                  .map(lambda i: Row(single=i, double=i ** 2)))\n",
    "squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "\n",
    "# Create another DataFrame in a new partition directory,\n",
    "# adding a new column and dropping an existing column\n",
    "cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "cubesDF.write.parquet(\"data/test_table/key=2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the partitioned table\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\n",
    "mergedDF.printSchema()\n",
    "mergedDF.show()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "result_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "print(distData.reduce(lambda a, b: a + b))\n",
    "print(distData.map(lambda a: a ** 2).reduce(lambda a,b: a+b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile = sc.textFile(\"../nlp/stanfordSentimentTreebank/dictionary*.txt\")\n",
    "distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "# conf = SparkConf()\n",
    "# conf.setMaster('spark://192.168.1.28:7077')\n",
    "# conf.setAppName('mynewapp2')\n",
    "# sc2 = SparkContext(conf=conf)\n",
    "# print (sc2)\n",
    "\n",
    "# def mod(x):\n",
    "#     import numpy as np\n",
    "#     return (x, np.mod(x, 2))\n",
    "# rdd = sc2.parallelize(range(100)).map(mod).take(10)\n",
    "# rdd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
