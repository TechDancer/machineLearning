{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Linear Regression\n",
    "\n",
    "Using the same example from Pierre, same data, but we can try to do this using basic machine learning.\n",
    "https://github.com/axiomiety/crashburn/blob/master/jupyter/simple_linear_regression.ipynb\n",
    "\n",
    "### Import the same datasets (and other setup libraries/functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pandas, io\n",
    "import time, itertools\n",
    "from myutils import *\n",
    "\n",
    "url='http://www.stat.ufl.edu/~winner/data/brainhead.dat'\n",
    "data=requests.get(url)\n",
    "col_names=('gender', 'age_range', 'head_size', 'brain_weight')\n",
    "col_widths=[(8,8),(16,16),(21-24),(29-32)]\n",
    "df=pandas.read_fwf(io.StringIO(data.text), names=col_names, colspec=col_widths)\n",
    "df.head()\n",
    "\n",
    "dfs = [df, churn(df,4),churn(df,8),churn(df,12),churn(df,16)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First before I start I want to do some sizing tests..  like a few magnitudes more data,\n",
    "so we make an array of dataFrames dfs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfs[0-4] #rows:\n",
      "237\n",
      "3792\n",
      "60672\n",
      "970752\n",
      "15532032\n"
     ]
    }
   ],
   "source": [
    "print('dfs[0-4] #rows:')\n",
    "for d in dfs:\n",
    "    print (d.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok I think these 5 samples is enough.\n",
    "\n",
    "## Analytical Method (and Performance) \n",
    "\n",
    "Looking at the builtin scipy regressino library, we see how fast it can solve a linear regression fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>sec</th>\n",
       "      <th>rows/sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237</td>\n",
       "      <td>4.87</td>\n",
       "      <td>48.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3792</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3717.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60672</td>\n",
       "      <td>2.44</td>\n",
       "      <td>24865.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>970752</td>\n",
       "      <td>44.36</td>\n",
       "      <td>21883.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15532032</td>\n",
       "      <td>1389.72</td>\n",
       "      <td>11176.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rows      sec  rows/sec\n",
       "0       237     4.87     48.67\n",
       "1      3792     1.02   3717.65\n",
       "2     60672     2.44  24865.57\n",
       "3    970752    44.36  21883.50\n",
       "4  15532032  1389.72  11176.38"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinregressResult(slope=0.2634293394893993, intercept=325.5734210494428, rvalue=0.7995697092542964, pvalue=5.957630839405777e-54, stderr=0.01290743344088697)\n",
      "LinregressResult(slope=0.2634293394893994, intercept=325.57342104944235, rvalue=0.7995697092542965, pvalue=0.0, stderr=0.0032140617796317618)\n",
      "LinregressResult(slope=0.2634293394893996, intercept=325.57342104944155, rvalue=0.7995697092542972, pvalue=0.0, stderr=0.0008033167598577374)\n",
      "LinregressResult(slope=0.26342933948939584, intercept=325.5734210494553, rvalue=0.7995697092542875, pvalue=0.0, stderr=0.0002008260867338187)\n",
      "LinregressResult(slope=0.26342933948941333, intercept=325.57342104939175, rvalue=0.7995697092543035, pvalue=0.0, stderr=5.020647319667088e-05)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "timings = {'rows':[],'sec':[]}\n",
    "for d in dfs:\n",
    "    r = time_fn(linregress,d.head_size,d.brain_weight)\n",
    "    timings['rows'].append(d.shape[0])\n",
    "    timings['sec'].append(round(r[1]*1000,2))\n",
    "t = pandas.DataFrame(timings)\n",
    "t['rows/sec'] = (t['rows']/t['sec']).round(2)\n",
    "display(t)\n",
    "\n",
    "for d in dfs:\n",
    "    print(linregress(d.head_size,d.brain_weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The perf table shows it isn't bad.  It is scaling better than I thought.  \n",
    "\n",
    "Doesn't prove what everyone says \"analytic solvers are good for < 10k, > 10k gradient descent solvers are better\".  Even at 15m points it solves it fast.   (??)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Sidebar\n",
    "For fun, wanted to compare performance w/ Pierre's solution (vs the scipy library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325.57342104944223, 0.26342933948939945]\n",
      "[325.5734210494413, 0.26342933948939967]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>sec</th>\n",
       "      <th>rows/sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237</td>\n",
       "      <td>30.672</td>\n",
       "      <td>7.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3792</td>\n",
       "      <td>383.045</td>\n",
       "      <td>9.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60672</td>\n",
       "      <td>5096.260</td>\n",
       "      <td>11.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>970752</td>\n",
       "      <td>78688.544</td>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rows        sec  rows/sec\n",
       "0     237     30.672      7.73\n",
       "1    3792    383.045      9.90\n",
       "2   60672   5096.260     11.91\n",
       "3  970752  78688.544     12.34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pierre_solver (df):\n",
    "    brain_weight_sample_mean = df['brain_weight'].mean()\n",
    "    head_size_sample_mean = df['head_size'].mean()\n",
    "    b_1 = sum(df.apply(lambda r: (r['head_size']-head_size_sample_mean)*(r['brain_weight']-brain_weight_sample_mean), axis=1))\n",
    "    b_1 /= sum(df.apply(lambda r: (r['head_size']-head_size_sample_mean)**2, axis=1))\n",
    "    b_0 = brain_weight_sample_mean-b_1*head_size_sample_mean\n",
    "    return [b_0, b_1]\n",
    "\n",
    "# prove the solutions are about same:\n",
    "print (pierre_solver(dfs[0]))\n",
    "print (pierre_solver(dfs[1]))\n",
    "\n",
    "# run first 2 \n",
    "timings = {'rows':[],'sec':[]}\n",
    "for d in dfs[:4]:\n",
    "    r = time_fn(pierre_solver,d)\n",
    "    timings['rows'].append(d.shape[0])\n",
    "    timings['sec'].append(r[1]*1000)\n",
    "t = pandas.DataFrame(timings)\n",
    "t['rows/sec'] = (t['rows']/t['sec']).round(2)\n",
    "display(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alot slower for obvious reasons (demo code, unoptimized etc).  However it does scale almost linearly.\n",
    "\n",
    "# Training Method \n",
    "\n",
    "Things needed for linear a regression line, of form f(x) = Ax + B (activation function)  \n",
    "- Find optimal A, B values & cost function J(A,B)  \n",
    "- Need partial derivatives for J(A,B) - dA and dB   \n",
    "- Iterate A = A + u * dA && B = B + u * dB  \n",
    "- Pretty much can start at random pt or 0,0 and do Gradient Descent  \n",
    "- Step until cost (J(A,B) is close to 0 or stops moving - define step size (u) and when to stop  \n",
    "\n",
    "First a basic gradient descent example for a simple function f (x^2 - 2x + 1)\n",
    "The derivative of this is 2x - 2, start at x=0, and solve where y=0\n",
    "\n",
    "### Gradient Descent Dummy Sample (1)\n",
    "\n",
    "This simple example below shows how a solver can work.  Take the simple f(x) = x^2 - 2x + 1, we all know from high school math the solution to x is (x-1)^2 =0 or x=1.  Gradient Descent (GD) is not unlike Newton's method, or any other guess-retry solver.  You start with a guess for X and iterate and get closer to the target solution.\n",
    "\n",
    "You need 2 things:\n",
    "- calculate a loss function on the X-guess\n",
    "- optimize the loss function (minimize) by adjusting the X-guess (by the derivative) \n",
    "\n",
    "Solving for x is just an iteration.  Start w/ a guess X=0, calculate the loss as f(X) vs the target answer: 0.  Iterate by adjusting X by df/dX (derivative) * a step factor.  (In theory you \"descend\" down the cost curve this way).  Eventually after n-iterations this will converge near 1.000 and have a cost near 0.000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= 0.2 y= 0.64\n",
      "x= 0.36 y= 0.4096\n",
      "x= 0.488 y= 0.262144\n",
      "x= 0.5904 y= 0.16777216\n",
      "x= 0.67232 y= 0.1073741824\n",
      "x= 0.737856 y= 0.068719476736\n",
      "x= 0.7902848 y= 0.043980465111\n",
      "x= 0.83222784 y= 0.0281474976711\n",
      "x= 0.865782272 y= 0.0180143985095\n",
      "x= 0.8926258176 y= 0.0115292150461\n",
      "x= 0.91410065408 y= 0.00737869762948\n",
      "error:  0.00737869762948\n",
      "x:  0.91410065408\n"
     ]
    }
   ],
   "source": [
    "from scipy.misc import derivative\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 2*x + 1\n",
    "\n",
    "def p(f, x):\n",
    "    return derivative(f,x)\n",
    "\n",
    "def grad_descent(f):\n",
    "    x = 0\n",
    "    step = 0.1\n",
    "    error = 0.5\n",
    "\n",
    "    # loop while error > err_lmt\n",
    "    while (error > 0.01):\n",
    "        x = x - step*(p(f,x)) \n",
    "        print ('x=',x, 'y=',f(x))\n",
    "        error = abs(f(x)-0)\n",
    "    print('error: ',error)\n",
    "    print('x: ',x)\n",
    "\n",
    "grad_descent(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together for a real example (Using HeadSize f(x) -> Brain Weight (y)\n",
    "\n",
    "Training set: x<sup>n</sup> ~ df['head_size']   \n",
    "Solution set: y<sup>n</sup> ~ df['brain_weight']   \n",
    "Hypothesis: h(x):  Ax + B   \n",
    "Cost P(h<sup>x</sup>) = P(A,B) = 1/n * sum( h(x<sup>n</sup>) - y<sup>n</sup> )^2   \n",
    "\n",
    "Consider P(A,B) = contour/3d of all costs of every combination A,B (0x+0, 1x+1, 0x+1, etc...)  \n",
    "Optimal is where P(A,B) = minimum  \n",
    "\n",
    "Start with guess Optimal P = A=1, B=1, h(x) = 1x+1  \n",
    "\n",
    "Optimal P(A,B) =  \n",
    " * A = A - step * dP/dA  <- initial guess +/- partial derivative of A (ie, movement of A parameter)      \n",
    " * B = B - step * dP/dA  <- initial guess +/- partial derivative of B (ie, movement of B parameter)  \n",
    "Repeat until close to step_limit  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init guess A: 1.000000, B: 1.000000\n",
      "init func: A*x + B, test size: 237\n",
      "init costF 0.00421940928270042*(2720.0*A + B - 955.0)**2 + 0.00421940928270042*(2773.0*A + \n",
      "init cost 5609738.70886076\n",
      "i=0,cost=3871290,A=0.135457,B=-1175.059072\n",
      "i=1,cost=2672945,A=0.851485,B=-192.217064\n",
      "i=2,cost=1846896,A=0.255257,B=-1001.816037\n",
      "i=3,cost=1277474,A=0.748530,B=-323.272384\n",
      "i=4,cost=884947,A=0.337256,B=-880.275431\n",
      "finished for rows,time(s) 15532032 14.868650000000002\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from sympy.core.compatibility import as_int\n",
    "import sympy.concrete.summations as sum\n",
    "import itertools\n",
    "\n",
    "# evaluate/calculate f with data sub for x and y (very slow iterative)\n",
    "def evalSumF(f,x,y,testData):\n",
    "    n=0\n",
    "    for _,d in testData.iterrows():  # global test data\n",
    "        n += f.subs(x,d.head_size).subs(y,d.brain_weight)\n",
    "    return n * (1.0/len(testData))\n",
    "\n",
    "# generate partial derivative of e, with respect to v, for testData (x,y) and evaluate\n",
    "def evalPartialDeriv(e,x,y,testData,v,guessV,o,guessO):\n",
    "    pc = evalSumF(sp.diff(e,v),x,y,testData)\n",
    "    pceval = pc.subs(v,guessV).subs(o,guessO)\n",
    "    return pceval\n",
    "\n",
    "# hard coded solver, start w/ guess, solve cost, iterate cost+/-partialDerivs\n",
    "def grad_descent2(testData):\n",
    "    guessA = guessB = 1.0   #initial guess y=1x+1\n",
    "\n",
    "    stepA = 0.00000005   #dif step for diff A,B ?\n",
    "    stepB = 0.25         #maybe normalize data first\n",
    "    step_limit = 0.0001  # when to stop, when cost stops changing\n",
    "    loop_limit = 5       # arbitrary max limits\n",
    "    costChange = 1.0\n",
    "\n",
    "    A,B,x,y = sp.symbols('A B x y')\n",
    "    f = A*x + B  # linear func y=mx+b\n",
    "    e = (f - y)**2  # error squared\n",
    "    print ('init guess A: %f, B: %f'%(guessA,guessB))\n",
    "    print ('init func: %s, test size: %d' %(str(f),testData.shape[0]))\n",
    "    \n",
    "    costF = evalSumF(e,x,y,testData)  # cost fun evaluted for testData\n",
    "    print('init costF',str(costF)[:80])\n",
    "    costEval = costF.subs(A,guessA).subs(B,guessB)  # cost evaluted for A B guess\n",
    "    print('init cost',costEval)\n",
    "\n",
    "    i=0  \n",
    "    while (abs(costChange) > step_limit and i<loop_limit):  # arbitrary limiter\n",
    "        pda = evalPartialDeriv(e,x,y,testData,A,guessA,B,guessB)\n",
    "        pdb = evalPartialDeriv(e,x,y,testData,B,guessB,A,guessA)\n",
    "        guessA = guessA - stepA * pda\n",
    "        guessB = guessB - stepB * pdb\n",
    "        previousCost = costEval\n",
    "        costEval = costF.subs(A,guessA).subs(B,guessB)\n",
    "        costChange = previousCost-costEval\n",
    "        print ('i=%d,cost=%d,A=%f,B=%f'%(i, int(costEval), guessA, guessB))\n",
    "        i=i+1\n",
    "    return guessA,guessB\n",
    "\n",
    "timings = []\n",
    "r = time_fn(grad_descent2,df)\n",
    "print ('finished for rows,time(s)',d.shape[0], r[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows it getting closer after 5 test iterations - cost is decreasing and A,B params are converging (?) towards solution of 0.26x + 325.   But this is super slow (32sec for 5 iterations)... and guess what, I ran this to near completion and it did converge after 1800 iterations!!  \n",
    "\n",
    "\n",
    "Other observations  \n",
    "- the initial A,B didn't matter as much as you'd think  \n",
    "- the step sizes for A,B really matter - wrong steps are you step widely over the solution back and forth.  Given A target range for a slope is small, and Y intercept is large, maybe we need to normalize/scale first?   Random fine tuning steps was required.  \n",
    "- it is super slow, in theory you can find a solution for large testData sets this way where analytic solutions wont work.  But my lame code using sympy is so slow.  Have to find a better library or more to matrix multiplication (not sure if I'll rewrite (gradient_descent3) using matrices but I should).  \n",
    "\n",
    "Visually showing the solution forming is interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations\n",
    "\n",
    "Gradient Descent (aka Batch Gradient Descent), while in theory faster than solving via the the normal equation (examples to come later), or the builtin solvers when the # of features or size of training set is very large, is still very slow overall.  *In Batch GD, every step takes the cost down and steps towards the final solution, since solving the derivatives for all test data always points you in the right direction*\n",
    " \n",
    "There are 2 optimizations to the GD algo above  \n",
    "\n",
    "1.  Stochastic GD   \n",
    "    + Step A - Add a shuffle to the test data set\n",
    "    + Step B - instead of solving the partial derivative for all test data, solve for just 1 and take a step  \n",
    "    + *Solving derivatives for just random testData doesn't always take your cost down, but testing and logic shows it   jumps around a little but get you there quickly*  \n",
    "    + *Each step is way faster like 1/n times faster where n is size of test data*  \n",
    "\n",
    "\n",
    "2.  Mini-Batch GD  \n",
    "    * Step A - Add a shuffle to the test data set\n",
    "    * Step B - instead of solving the partial derivative for all test data, solve for a small batch like 10 and take a  step  \n",
    "    * *Solving a small batch makes it more likely to step in the right directin (cost down), and testing agrees*\n",
    "    * *Each step is also way faster like batchSize/n times faster* \n",
    "\n",
    "Sample results of a test run of 1,000 iterations of standard B-GD, S-GD, and MB-GD (size 10) for test data size of 200\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><td>*</td><td>B-GD</td><td>MB-GD</td><td>S-GD</td></tr>\n",
    "    <tr><td>A</td><td>0.28</td><td>0.27</td><td>0.19</td></tr>\n",
    "    <tr><td>B</td><td>253</td><td>365</td><td>477</td></tr>\n",
    "    <tr><td>Timing(s)</td><td>1899</td><td>702</td><td>582</td></tr>\n",
    "</table>\n",
    "\n",
    "(Reminder target solutions are A = 0.26, B=325)\n",
    "\n",
    "I should graph the results because its not evident from above that the results of MB-GD and S-GD actually jump around close  to the solution for some time but do not converge very well, while B-GD always moves slowly towards the final answer.   Some fine tuning required I think, as MB-GD and S-GD don't need 1,000 iterations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation - Linear Algebra\n",
    "\n",
    "Final note, the Normal Equation can be used which is really quite easy as shown below using numpy to calculate\n",
    "\n",
    "![Normal Equation Image](https://csharpcorner-mindcrackerinc.netdna-ssl.com/article/normal-equation-implementation-from-scratch-in-python/Images/image001.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -2.48689958e-14],\n",
       "        [  1.00000000e+01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "\n",
    "X = np.matrix('1,1;1,2;1,4;1,5')     # make sample matrix of test X head size data, note u have to put a dummy 1 column\n",
    "Y = np.matrix('10;20;40;50')         # make sample matrix of correlated Y(X) brain weight data\n",
    "\n",
    "(X.T.dot(X)).I.dot(X.T).dot(Y)       # We can expect a simple line of Y=10x+0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has some floating pt rounding issues, but its close to 0, 10 (y=10x+0).  To make it more realistic, we'll solve for the real data set as well using the original examples of data from http://www.stat.ufl.edu/~winner/data/brainhead.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3.25573421e+02, 2.63429339e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pandas.read_fwf(io.StringIO(data.text), names=col_names, colspec=col_widths)\n",
    "\n",
    "def normal_solver(d):\n",
    "    # some setup to make a ndarray\n",
    "    X = pandas.DataFrame({'Bias' : 1,'head_size' : d['head_size']})\n",
    "    Y = d['brain_weight']\n",
    "    X = np.asmatrix(X.as_matrix())\n",
    "    Y = Y.as_matrix()\n",
    "    return (X.T.dot(X)).I.dot(X.T).dot(Y)\n",
    "\n",
    "normal_solver(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look good, Y = 0.26x + 325 as expected !\n",
    "\n",
    "For kicks I timed the normal equation... as expected - Normal method slows down alot more than linear regression library.   Also note the Normal method does not work for all equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linegresstime</th>\n",
       "      <th>normaltime</th>\n",
       "      <th>rows</th>\n",
       "      <th>Normal / sec</th>\n",
       "      <th>LinRegress / sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.517</td>\n",
       "      <td>18.740</td>\n",
       "      <td>237</td>\n",
       "      <td>12.65</td>\n",
       "      <td>52.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.986</td>\n",
       "      <td>3.198</td>\n",
       "      <td>3792</td>\n",
       "      <td>1185.74</td>\n",
       "      <td>3845.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.711</td>\n",
       "      <td>11.579</td>\n",
       "      <td>60672</td>\n",
       "      <td>5239.83</td>\n",
       "      <td>35459.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.766</td>\n",
       "      <td>110.714</td>\n",
       "      <td>970752</td>\n",
       "      <td>8768.11</td>\n",
       "      <td>31552.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>520.948</td>\n",
       "      <td>2355.387</td>\n",
       "      <td>15532032</td>\n",
       "      <td>6594.26</td>\n",
       "      <td>29814.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   linegresstime  normaltime      rows  Normal / sec  LinRegress / sec\n",
       "0          4.517      18.740       237         12.65             52.47\n",
       "1          0.986       3.198      3792       1185.74           3845.84\n",
       "2          1.711      11.579     60672       5239.83          35459.96\n",
       "3         30.766     110.714    970752       8768.11          31552.75\n",
       "4        520.948    2355.387  15532032       6594.26          29814.94"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timings = {'rows':[],'normaltime':[],'linegresstime':[]}\n",
    "for d in dfs:\n",
    "    r  = time_fn(normal_solver,d)\n",
    "    r2 = time_fn(linregress,d.head_size,d.brain_weight)\n",
    "    timings['rows'].append(d.shape[0])\n",
    "    timings['normaltime'].append(r[1]*1000)\n",
    "    timings['linegresstime'].append(r2[1]*1000)\n",
    "t = pandas.DataFrame(timings)\n",
    "t['Normal / sec'] = (t['rows']/t['normaltime']).round(2)\n",
    "t['LinRegress / sec'] = (t['rows']/t['linegresstime']).round(2)\n",
    "\n",
    "display(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
